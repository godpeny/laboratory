{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Using same data preprocessing and tokenizing as “naver_movie_review_sentiment_analysis” in “Recurrent_Neural_Network_Text_Classification”"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f810dd1424da95c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from konlpy.tag import Okt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename= data_path + \"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=data_path + \"ratings_test.txt\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b654f4c357eef35e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = pd.read_table(data_path + \"ratings_train.txt\")\n",
    "test_data = pd.read_table(data_path + \"ratings_test.txt\")\n",
    "print(train_data[:3])\n",
    "print(test_data[:3])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0d7d471549855d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4efd9ed4dde5d1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check null and unique values\n",
    "print(len(train_data))\n",
    "print(train_data.isnull().values.any())\n",
    "print(train_data.nunique()) # count number of unique values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f207beb2a7e73fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove null values and duplicates\n",
    "train_data.dropna(inplace=True) \n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) \n",
    "\n",
    "print(train_data.isnull().values.any())\n",
    "print(train_data.nunique())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1a2893bbf75ec5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data[\"label\"].value_counts().plot(kind='bar')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e75fa698d0eef83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data[\"document\"] = train_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
    "train_data[:5]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89ace2098825d48b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data[\"document\"].replace(\"^ +\", \"\", regex=True, inplace=True) # remove leading whitespace. e.g., \"  안녕하세요\" -> \"안녕하세요\"\n",
    "train_data[\"document\"].replace(\"\", np.nan, inplace=True) # replace empty string with null\n",
    "\n",
    "print(train_data[\"document\"].isnull().sum())\n",
    "train_data.dropna(inplace=True)\n",
    "\n",
    "print(len(train_data))\n",
    "print(train_data[\"document\"].isnull().sum())\n",
    "print(train_data.isnull().values.any())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fc7de3e21188524"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# apply same preprocessing to test data\n",
    "test_data.dropna(inplace=True)\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "test_data[\"document\"] = test_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
    "test_data[\"document\"].replace(\"^ +\", \"\", regex=True, inplace=True) # remove leading whitespace. e.g., \"  안녕하세요\" -> \"안녕하세요\"\n",
    "test_data[\"document\"].replace(\"\", np.nan, inplace=True) # replace empty string with null\n",
    "test_data.dropna(inplace=True)\n",
    "\n",
    "print(len(test_data))\n",
    "print(test_data[\"document\"].isnull().sum())\n",
    "print(test_data.isnull().values.any())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2565917ae1cd293a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f39a8fc9662acdd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "X_train = []\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# tokenize and remove stopwords from sentences\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized = okt.morphs(sentence, stem=True) # tokenize\n",
    "    stopwords_removed = [word for word in tokenized if not word in stopwords] # remove stopwords\n",
    "    X_train.append(stopwords_removed)\n",
    "    \n",
    "print(X_train[:5])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8920de9cef2d501b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test = []\n",
    "\n",
    "for sentence in tqdm(test_data['document']):\n",
    "    tokenized = okt.morphs(sentence, stem=True) # tokenize\n",
    "    stopwords_removed = [word for word in tokenized if not word in stopwords] # remove stopwords\n",
    "    X_test.append(stopwords_removed)\n",
    "    \n",
    "print(X_test[:5])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee5619b261a2fcc5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# encode words to integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "print(tokenizer.word_index)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "905ae315b966c13d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = 3 # set threshold for rare words\n",
    "total_freq = 0\n",
    "total_word_num = len(tokenizer.word_index)\n",
    "rare_freq = 0\n",
    "rare_word_num = 0\n",
    "\n",
    "for word, cnt in tokenizer.word_counts.items():\n",
    "    total_freq += cnt\n",
    "    \n",
    "    if cnt < threshold:\n",
    "        rare_freq += cnt\n",
    "        rare_word_num += 1\n",
    "        \n",
    "        \n",
    "print(\"Total number of words: \", total_freq)\n",
    "print(\"Number of rare words: \", rare_word_num)\n",
    "print(\"Percentage of rare words: \", (rare_word_num / total_word_num) * 100)\n",
    "print(\"Percentage of rare words in total frequency: \", (rare_freq / total_freq) * 100)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9d16b60ca1ecdf0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_size = total_word_num - rare_word_num +1\n",
    "print(vocab_size)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fc20555671b1964"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ReTokenize with vocab_size\n",
    "tokenizer = Tokenizer(vocab_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "print(X_train_seq[:5])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32024475a31e381c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_seq = tokenizer.texts_to_sequences(X_test) # skip fit_on_texts because it's already fitted\n",
    "print(X_test_seq[:5])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59fd7d9e11029746"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6022a9e50c39b90f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Additional Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaf2809adfdb997"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove empty sentences after removing low-frequency words\n",
    "print(len(X_train_seq), len(y_train))\n",
    "drop_train_idx = [index for index, sentence in enumerate(X_train_seq) if len(sentence) < 1]\n",
    "print(len(drop_train_idx))\n",
    "X_train_seq_removed = [sentence for index, sentence in enumerate(X_train_seq) if index not in drop_train_idx]\n",
    "y_train_removed = [label for index, label in enumerate(y_train) if index not in drop_train_idx]\n",
    "print(len(X_train_seq_removed), len(y_train_removed))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "846df4e3b8898e13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### numpy.delete is not working with different length of sub arrays in 2d array"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4748d8d0e1947e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# padding\n",
    "max_len = max(len(sentence) for sentence in X_train_seq_removed)\n",
    "avg_len = sum(map(len, X_train_seq_removed)) / len(X_train_seq_removed)\n",
    "print(max_len, avg_len)\n",
    "\n",
    "plt.hist([len(sentence) for sentence in X_train_seq_removed], bins=50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4ad23301cb81893"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set max_len to 30~40 based on the histogram\n",
    "max_padding = 30\n",
    "\n",
    "# make list to numpy array\n",
    "X_train_seq_padded = pad_sequences(X_train_seq_removed, maxlen=max_padding)\n",
    "X_test_seq_padded = pad_sequences(X_test_seq, maxlen=max_padding)\n",
    "\n",
    "y_train_removed = np.array(y_train_removed)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(X_train_seq_padded.shape, X_test_seq_padded.shape)\n",
    "print(y_train_removed.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1734fc11fd712f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e0cfee34bbad88b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, Input, Flatten, Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "382c141e053a5aab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "embedding_dim = 128\n",
    "dropout_ratio = (0.5, 0.8) # use two dropout layers\n",
    "num_filters = 128\n",
    "hidden_units = 128\n",
    "\n",
    "convs = []\n",
    "\n",
    "input = Input(shape=(max_padding,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_padding)(input)\n",
    "embedding_dropped_out = Dropout(dropout_ratio[0])(embedding)\n",
    "\n",
    "# use different kernel sizes for convolution\n",
    "for size in [3,4,5]:\n",
    "    conv = Conv1D(filters=num_filters, kernel_size=size, padding='valid', activation='relu', strides=1)(embedding_dropped_out)\n",
    "    conv_pooled = GlobalMaxPooling1D()(conv)\n",
    "    convs.append(conv_pooled)\n",
    "\n",
    "output = Concatenate()(convs)\n",
    "output = Dropout(dropout_ratio[1])(output)\n",
    "output = Dense(hidden_units, activation='relu')(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(inputs=input, outputs=output)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6d36bc9fbf347b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path = \"../model/\"\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint(filepath=model_path+'review_best_model_cnn.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train_seq_padded, y_train_removed, epochs=10, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b02d42681cfab3d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loaded_model = load_model(model_path+'review_best_model_cnn.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test_seq_padded, y_test)[1]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eba397e98ed1231"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bb9d439909135b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(new_sentence):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_padding) # 패딩\n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e1f8ae3dde67b5e"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n",
      "89.29% 확률로 긍정 리뷰입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/76/r353dd3n1cb9npy26cwjvyz00000gn/T/ipykernel_1889/389656473.py:7: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  score = float(loaded_model.predict(pad_new)) # 예측\n"
     ]
    }
   ],
   "source": [
    "predict(\"이 영화 개꿀잼 ㅋㅋㅋ\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T03:17:01.474753Z",
     "start_time": "2023-11-18T03:17:01.445713Z"
    }
   },
   "id": "a2ce0a7578174c50"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
