{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-23T12:37:12.451949Z",
     "start_time": "2023-12-23T12:37:07.996125Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-Head Attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8d2755cc5733cc1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(keras.layers.Layer):\n",
    "    def __init__(self, embedding_dims, num_heads=8):\n",
    "        super(Multi_Head_Attention, self).__init__()\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dims = embedding_dims // num_heads\n",
    "        \n",
    "        self.WQ = keras.layers.Dense(embedding_dims)\n",
    "        self.WK = keras.layers.Dense(embedding_dims)\n",
    "        self.WV = keras.layers.Dense(embedding_dims)\n",
    "        self.WO = keras.layers.Dense(embedding_dims)\n",
    "        \n",
    "    def scaled_dot_attention(self, query, key, value):\n",
    "        # query, key, value shape: (batch_size, num_heads, seq_len, head_dims)\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32) # last dimension of key\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits) # softmax is normalized on the last axis\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        # split the last dimension into (num_heads, head_dims)\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dims))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) # permute the dimensions into (batch_size, num_heads, seq_len, head_dims)\n",
    "    \n",
    "    def call(self, input):\n",
    "        # input shape: (batch_size, seq_len, embedding_dims)\n",
    "        batch_size = tf.shape(input)[0]\n",
    "        \n",
    "        query = self.WQ(input) # (batch_size, seq_len, embedding_dims)\n",
    "        key = self.WK(input) # (batch_size, seq_len, embedding_dims)\n",
    "        value = self.WV(input) # (batch_size, seq_len, embedding_dims)\n",
    "        \n",
    "        query = self.split_heads(query, batch_size) # (batch_size, num_heads, seq_len, head_dims)\n",
    "        key = self.split_heads(key, batch_size) # (batch_size, num_heads, seq_len, head_dims)\n",
    "        value = self.split_heads(value, batch_size) # (batch_size, num_heads, seq_len, head_dims)\n",
    "        \n",
    "        scaled_attention, attention_weights = self.scaled_dot_attention(query, key, value)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # restore the original dimensions (batch_size, seq_len, num_heads, head_dims)\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dims)) # (batch_size, seq_len, embedding_dims)\n",
    "        output = self.WO(concat_attention) # (batch_size, seq_len, embedding_dims)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T12:37:12.456355Z",
     "start_time": "2023-12-23T12:37:12.454212Z"
    }
   },
   "id": "ee91066a3883fef2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dims, num_heads, ff_dims, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = Multi_Head_Attention(embedding_dims, num_heads)\n",
    "        self.ffn = keras.Sequential(layers=[\n",
    "            keras.layers.Dense(units=ff_dims, activation='relu'),\n",
    "            keras.layers.Dense(units=embedding_dims)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "    def call(self, input, training):\n",
    "        attention_output = self.att(input) \n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        attention_value = self.layernorm1(input + attention_output)\n",
    "        \n",
    "        ffn_output = self.ffn(attention_value)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        final_output = self.layernorm2(attention_value + ffn_output)\n",
    "        return final_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T12:37:12.470538Z",
     "start_time": "2023-12-23T12:37:12.457909Z"
    }
   },
   "id": "4e2e2717b4af9fb7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tf.keras.layers\n",
    " - `call(self, inputs, *args, **kwargs)`: Called in `__call__` after making\n",
    "      sure `build()` has been called. `call()` performs the logic of applying\n",
    "      the layer to the `inputs`. The first invocation may additionally create\n",
    "      state that could not be conveniently created in `build()`; see its\n",
    "      docstring for details.\n",
    "      Two reserved keyword arguments you can optionally use in `call()` are:\n",
    "        - `training` (boolean, whether the call is in inference mode or training\n",
    "          mode). See more details in [the layer/model subclassing guide](\n",
    "          https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method)\n",
    "        - `mask` (boolean tensor encoding masked timesteps in the input, used\n",
    "          in RNN layers). See more details in\n",
    "          [the layer/model subclassing guide](\n",
    "          https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_mask_argument_in_the_call_method)\n",
    "      A typical signature for this method is `call(self, inputs)`, and user\n",
    "      could optionally add `training` and `mask` if the layer need them. `*args`\n",
    "      and `**kwargs` is only useful for future extension when more input\n",
    "      parameters are planned to be added."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d366be150b1ae72"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Token_And_Position_Embedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embedding_dims):\n",
    "        super(Token_And_Position_Embedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dims) # (none, none, embedding_dims)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embedding_dims) # (none, none, embedding_dims)\n",
    "                \n",
    "    def call(self, input):\n",
    "        limit = tf.shape(input)[-1]\n",
    "        positions = tf.range(start=0, limit=limit, delta=1) # (maxlen,)\n",
    "        positions = self.pos_emb(positions) # (maxlen, embedding_dims)\n",
    "        token = self.token_emb(input) # (batch_size, limit, embedding_dims)\n",
    "        \n",
    "        return token + positions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T12:37:12.474494Z",
     "start_time": "2023-12-23T12:37:12.472103Z"
    }
   },
   "id": "fb3a01ecacd499b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "maxlen = 100\n",
    "positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "print(positions) # tf.Tensor([ 0  1  2 ... 97 98 99], shape=(100,), dtype=int32)\n",
    "positions_embedding = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=512)(positions)\n",
    "print(positions_embedding) # tf.Tensor(..., shape=(100, 512), dtype=float32))\n",
    "print(positions.shape, positions_embedding.shape) # (100,) (100, 512)\n",
    "```\n",
    "\n",
    "### layer.embedding\n",
    "```python\n",
    "model.add(tf.keras.layers.Embedding(input_dims=1000, output_dims=64, input_length=10))\n",
    "# The model will take as input an integer matrix of size (batch, input_length).\n",
    "# And the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# Now model.output_shape is (None, 10, 64), where `None` is the batch\n",
    "```\n",
    "```python\n",
    "print('vocab_size: ', vocab_size) # 20000\n",
    "print('max_len: ', max_len) # 200\n",
    "print('embedding_dims: ', embedding_dims) # 32\n",
    "\n",
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dims, input_length=max_len)) # (None, 200, 32)\n",
    "# model2.add(keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dims)) # (None, None, 32)\n",
    "model2.summary()\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e94a67b64128b91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "310879300c2a9baf"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 리뷰 개수 : 25000\n",
      "테스트용 리뷰 개수 : 25000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000  # 빈도수 상위 2만개의 단어만 사용\n",
    "max_len = 200  # 문장의 최대 길이\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print('훈련용 리뷰 개수 : {}'.format(len(X_train)))\n",
    "print('테스트용 리뷰 개수 : {}'.format(len(X_test)))\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T12:37:14.176412Z",
     "start_time": "2023-12-23T12:37:12.474957Z"
    }
   },
   "id": "a74c1acbaf7dc5ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d7aad5b041a7f43"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from keras.layers import Input, GlobalAveragePooling1D, Dropout, Dense"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T12:37:14.181442Z",
     "start_time": "2023-12-23T12:37:14.176558Z"
    }
   },
   "id": "642ef9014fc2b96b"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " token__and__position__embe  (None, 200, 32)           646400    \n",
      " dding (Token_And_Position_                                      \n",
      " Embedding)                                                      \n",
      "                                                                 \n",
      " transformer_block (Transfo  (None, 200, 32)           6464      \n",
      " rmerBlock)                                                      \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 32)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 653566 (2.49 MB)\n",
      "Trainable params: 653566 (2.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "embedding_dims = 32\n",
    "num_heads = 2\n",
    "ff_dims = 32 # hidden layer size in feed forward network inside transformer\n",
    "\n",
    "embedding = Token_And_Position_Embedding(max_len, vocab_size, embedding_dims)\n",
    "token_and_position_emb = TransformerBlock(embedding_dims, num_heads, ff_dims)\n",
    "\n",
    "input = Input(shape=(max_len,))\n",
    "\n",
    "x = embedding(input)\n",
    "x = token_and_position_emb(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=input, outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T12:37:14.414272Z",
     "start_time": "2023-12-23T12:37:14.180061Z"
    }
   },
   "id": "73e5d749b476fc58"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.3861 - accuracy: 0.8151 - val_loss: 0.2911 - val_accuracy: 0.8766\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.1969 - accuracy: 0.9257 - val_loss: 0.3447 - val_accuracy: 0.8674\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=2, validation_data=(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T12:38:10.533198Z",
     "start_time": "2023-12-23T12:37:14.415229Z"
    }
   },
   "id": "ff75a5128072ffbb"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 8s 10ms/step - loss: 0.3447 - accuracy: 0.8674\n",
      "테스트 정확도: 0.8674\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T12:38:18.597171Z",
     "start_time": "2023-12-23T12:38:10.530916Z"
    }
   },
   "id": "70ff3cbb328df090"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
