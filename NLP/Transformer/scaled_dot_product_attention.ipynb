{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-16T09:42:49.112469Z",
     "start_time": "2023-12-16T09:42:46.855177Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    \n",
    "    depth : d_model / num_heads\n",
    "    ... : batch_size\n",
    "    \n",
    "    Args:\n",
    "        query: query shape == (..., num_heads, seq_len_q, depth)\n",
    "        key: key shape == (..., num_heads, seq_len_k, depth)\n",
    "        value: value shape == (..., num_heads, seq_len_v, depth)     \n",
    "        mask : mask shape == (..., 1, 1, seq_len_k)   \n",
    "        \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "    \n",
    "    matmul_qk = tf.matmul(a=query, b=key, transpose_b=True)  # Q*K while K is transposed. (..., num_heads, seq_len_q, seq_len_k)\n",
    "    depth_float = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "    attention_logits = matmul_qk / tf.math.sqrt(depth_float)  # scale matmul_qk\n",
    "    \n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        attention_logits += (mask * -1e9)  # -1e9 : -infinite\n",
    "        \n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    # calculate the attention weights(== attention distribution).\n",
    "    attention_weights = tf.nn.softmax(attention_logits, axis=-1)  # (..., num_heads, seq_len_q, seq_len_k)\n",
    "    \n",
    "    attention_values = tf.matmul(attention_weights, value)  # (..., num_heads, seq_len_q, depth)\n",
    "    \n",
    "    return attention_values, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T10:51:02.468849Z",
     "start_time": "2023-12-16T10:51:02.461502Z"
    }
   },
   "id": "810149d2fc04591a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### logit : \n",
    " - 확률 값으로 변환되기 직전의 최종 결과 값 (== score)\n",
    " - 마지막 노드에서 아무런 Activation Function을 거치지 않은 값.\n",
    "\n",
    "### tf.matmul:\n",
    " - when calculating matrices with more than 2-D, last two dimensions are multiplied."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502db990ddd7b4cb"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T10:51:03.532648Z",
     "start_time": "2023-12-16T10:51:03.527890Z"
    }
   },
   "id": "ea6daee93376daca"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 함수 실행\n",
    "# temp_q == temp_k[1], temp_out == temp_v[1]\n",
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "print(temp_attn) # attention distribution (== attention weights)\n",
    "print(temp_out) # attention values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T10:50:53.096317Z",
     "start_time": "2023-12-16T10:50:53.088626Z"
    }
   },
   "id": "a85c176bab2d2e86"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# temp_q = temp_k[2], temp_k[3], temp_out = temp_attn[2]*temp_v[2] + temp_attn[3]*temp_v[3]\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)\n",
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "print(temp_attn)\n",
    "print(temp_out) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T10:47:05.671244Z",
     "start_time": "2023-12-16T10:47:05.655348Z"
    }
   },
   "id": "38922c0c762ab5cf"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "print(temp_attn) \n",
    "print(temp_out)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T10:49:37.382557Z",
     "start_time": "2023-12-16T10:49:37.365681Z"
    }
   },
   "id": "8467c5d4025a3788"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f61eff67a5517b2d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
