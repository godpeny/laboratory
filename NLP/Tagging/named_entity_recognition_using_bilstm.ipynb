{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-19T02:34:44.239748Z",
     "start_time": "2023-11-19T02:34:41.936458Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041\n",
      "[['eu', 'B-ORG'], ['rejects', 'O'], ['german', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['british', 'B-MISC'], ['lamb', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/\"\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/12.%20RNN%20Sequence%20Labeling/dataset/train.txt\", filename=data_path+\"train_tag.txt\")\n",
    "\n",
    "f = open(data_path+'train_tag.txt', 'r')\n",
    "tagged_sentences = []\n",
    "sentence = []\n",
    "\n",
    "for line in f:\n",
    "    if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "        if len(sentence) > 0:\n",
    "            tagged_sentences.append(sentence)\n",
    "            sentence = []\n",
    "        continue\n",
    "    splits = line.split(' ') # 공백을 기준으로 속성을 구분한다.\n",
    "    splits[-1] = re.sub(r'\\n', '', splits[-1]) # 줄바꿈 표시 \\n을 제거한다.\n",
    "    word = splits[0].lower() # 단어들은 소문자로 바꿔서 저장한다.\n",
    "    sentence.append([word, splits[-1]]) # 단어와 개체명 태깅만 기록한다.\n",
    "    \n",
    "print(len(tagged_sentences))\n",
    "print(tagged_sentences[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T02:41:52.792611Z",
     "start_time": "2023-11-19T02:41:52.313934Z"
    }
   },
   "id": "dbc1fd3df18587c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d034c52bd6c7cba"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentences, tags = [], []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sen, tag = zip(*tagged_sentence)\n",
    "    sentences.append(list(sen))\n",
    "    tags.append(list(tag))\n",
    "    \n",
    "print(sentences[0])\n",
    "print(tags[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T02:48:38.304334Z",
     "start_time": "2023-11-19T02:48:38.300316Z"
    }
   },
   "id": "97f269c25dc044b8"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 14.501887329962253\n"
     ]
    }
   ],
   "source": [
    "# check data distribution\n",
    "max_sentence_len = max(len(sentence) for sentence in sentences)\n",
    "avg_sentence_len = sum(map(len, sentences))/len(sentences)\n",
    "\n",
    "print(max_sentence_len, avg_sentence_len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T02:49:22.495606Z",
     "start_time": "2023-11-19T02:49:22.482515Z"
    }
   },
   "id": "c86e712d1f06fbce"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# check length of sentences\n",
    "def len_sen(num):\n",
    "    cnt = 0\n",
    "    for sentence in X_train:\n",
    "        if len(sentence) > num:\n",
    "            cnt += 1\n",
    "    print((1-(cnt/len(X_train))) * 100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T02:58:10.136672Z",
     "start_time": "2023-11-19T02:58:10.130238Z"
    }
   },
   "id": "de41304bcfb25976"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.98575600028488\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(len_sen(70)) # if padding with max 70 length, 99.98% of sentences are included\n",
    "max_len = 70"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T02:58:58.412505Z",
     "start_time": "2023-11-19T02:58:58.402848Z"
    }
   },
   "id": "4a28cb69d6d42b95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4363079872c2fae4"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21010\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "vocab_size = 4000 # use most frequent 4000 words\n",
    "\n",
    "sen_tokenizer = Tokenizer(num_words=vocab_size, oov_token='OOV')\n",
    "sen_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(tags)\n",
    "\n",
    "print(len(sen_tokenizer.word_index))\n",
    "print(len(tag_tokenizer.word_index))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T02:59:00.691822Z",
     "start_time": "2023-11-19T02:59:00.548358Z"
    }
   },
   "id": "f7fb8ec01665829c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[989, 1, 205, 629, 7, 3939, 216, 1, 3]\n",
      "[4, 1, 7, 1, 1, 1, 7, 1, 1]\n",
      "['eu', 'OOV', 'german', 'call', 'to', 'boycott', 'british', 'OOV', '.']\n"
     ]
    }
   ],
   "source": [
    "X_train = sen_tokenizer.texts_to_sequences(sentences)\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "decoded = []\n",
    "for idx in X_train[0]:\n",
    "    decoded.append(sen_tokenizer.index_word[idx])\n",
    "    \n",
    "print(decoded) # less frequent words are replaced with 'OOV' "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T02:59:01.777601Z",
     "start_time": "2023-11-19T02:59:01.753628Z"
    }
   },
   "id": "22d677aabf72460d"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11232, 70) (2809, 70)\n",
      "(11232, 70, 10) (2809, 70, 10)\n"
     ]
    }
   ],
   "source": [
    "# padding\n",
    "X_train_pad = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "y_train_pad = pad_sequences(y_train, padding='post', maxlen=max_len)\n",
    "\n",
    "# split train and test data\n",
    "X_train_pad_split, X_test_pad_split, y_train_pad_split, y_test_pad_split = train_test_split(X_train_pad, y_train_pad, test_size=0.2, random_state=777)\n",
    "\n",
    "# one-hot encoding\n",
    "y_train_pad_encoded = to_categorical(y_train_pad_split)\n",
    "y_test_pad_encoded = to_categorical(y_test_pad_split)\n",
    "\n",
    "print(X_train_pad_split.shape, X_test_pad_split.shape)\n",
    "print(y_train_pad_encoded.shape, y_test_pad_encoded.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T03:12:26.180974Z",
     "start_time": "2023-11-19T03:12:26.120675Z"
    }
   },
   "id": "de2e5dfbe04be84f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "488b1627f1c8c936"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\n",
    "from keras.optimizers import Adam"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T03:12:31.168318Z",
     "start_time": "2023-11-19T03:12:31.159901Z"
    }
   },
   "id": "4c2f6fd826c488c6"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 70, 128)           512000    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 70, 256)           263168    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 70, 10)            2570      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 777738 (2.97 MB)\n",
      "Trainable params: 777738 (2.97 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "embedding_dim = 128\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(units=hidden_units, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(units=len(tag_tokenizer.word_index)+1, activation='softmax')))\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T03:12:33.225816Z",
     "start_time": "2023-11-19T03:12:32.625267Z"
    }
   },
   "id": "13911cb087404511"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "71/71 [==============================] - 14s 201ms/step - loss: 0.9969 - accuracy: 0.8317 - val_loss: 0.6964 - val_accuracy: 0.8338\n",
      "Epoch 2/8\n",
      "71/71 [==============================] - 15s 210ms/step - loss: 0.5747 - accuracy: 0.8368 - val_loss: 0.4637 - val_accuracy: 0.8555\n",
      "Epoch 3/8\n",
      "71/71 [==============================] - 15s 219ms/step - loss: 0.4103 - accuracy: 0.8739 - val_loss: 0.3608 - val_accuracy: 0.8914\n",
      "Epoch 4/8\n",
      "71/71 [==============================] - 15s 217ms/step - loss: 0.3099 - accuracy: 0.9083 - val_loss: 0.2663 - val_accuracy: 0.9210\n",
      "Epoch 5/8\n",
      "71/71 [==============================] - 16s 224ms/step - loss: 0.2273 - accuracy: 0.9337 - val_loss: 0.2153 - val_accuracy: 0.9385\n",
      "Epoch 6/8\n",
      "71/71 [==============================] - 15s 218ms/step - loss: 0.1725 - accuracy: 0.9494 - val_loss: 0.1818 - val_accuracy: 0.9477\n",
      "Epoch 7/8\n",
      "71/71 [==============================] - 16s 227ms/step - loss: 0.1441 - accuracy: 0.9576 - val_loss: 0.1700 - val_accuracy: 0.9513\n",
      "Epoch 8/8\n",
      "71/71 [==============================] - 15s 219ms/step - loss: 0.1229 - accuracy: 0.9638 - val_loss: 0.1672 - val_accuracy: 0.9528\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_pad_split, y_train_pad_encoded, batch_size=128, epochs=8, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T03:14:55.452875Z",
     "start_time": "2023-11-19T03:12:52.513673Z"
    }
   },
   "id": "d6d7beee17680904"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 22ms/step - loss: 0.1707 - accuracy: 0.9526\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9526053071022034"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_pad_split, y_test_pad_encoded)[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T03:15:24.538184Z",
     "start_time": "2023-11-19T03:15:22.571954Z"
    }
   },
   "id": "43866e3ee551dea3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inferencing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "231c938c3970be57"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 849ms/step\n",
      "단어             |실제값  |예측값\n",
      "-----------------------------------\n",
      "sarah            : B-PER   B-PER\n",
      "brady            : I-PER   I-PER\n",
      ",                : O       O\n",
      "whose            : O       O\n",
      "republican       : B-MISC  B-MISC\n",
      "husband          : O       O\n",
      "was              : O       O\n",
      "OOV              : O       O\n",
      "OOV              : O       O\n",
      "in               : O       O\n",
      "an               : O       O\n",
      "OOV              : O       O\n",
      "attempt          : O       O\n",
      "on               : O       O\n",
      "president        : O       O\n",
      "ronald           : B-PER   B-PER\n",
      "reagan           : I-PER   I-PER\n",
      ",                : O       O\n",
      "took             : O       O\n",
      "centre           : O       O\n",
      "stage            : O       O\n",
      "at               : O       O\n",
      "the              : O       O\n",
      "democratic       : B-MISC  B-MISC\n",
      "national         : I-MISC  I-MISC\n",
      "convention       : I-MISC  I-MISC\n",
      "on               : O       O\n",
      "monday           : O       O\n",
      "night            : O       O\n",
      "to               : O       O\n",
      "OOV              : O       O\n",
      "president        : O       O\n",
      "bill             : B-PER   B-PER\n",
      "clinton          : I-PER   I-PER\n",
      "'s               : O       O\n",
      "gun              : O       O\n",
      "control          : O       O\n",
      "efforts          : O       O\n",
      ".                : O       O\n"
     ]
    }
   ],
   "source": [
    "i = 10 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
    "\n",
    "# 인덱싱\n",
    "index_to_word = sen_tokenizer.index_word\n",
    "index_to_ner = tag_tokenizer.index_word\n",
    "\n",
    "# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
    "y_predicted = model.predict(np.array([X_test_pad_split[i]]))\n",
    "\n",
    "# 확률 벡터를 정수 레이블로 변경.\n",
    "y_predicted = np.argmax(y_predicted, axis=-1)\n",
    "\n",
    "# 원-핫 벡터를 정수 인코딩으로 변경.\n",
    "labels = np.argmax(y_test_pad_encoded[i], -1)\n",
    "\n",
    "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
    "print(35 * \"-\")\n",
    "\n",
    "for word, tag, pred in zip(X_test_pad_split[i], labels, y_predicted[0]):\n",
    "    if word != 0: # PAD값은 제외함.\n",
    "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag].upper(), index_to_ner[pred].upper()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T03:18:49.951548Z",
     "start_time": "2023-11-19T03:18:49.077377Z"
    }
   },
   "id": "32f9ae128b7d95f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### O is the most frequent tag, so accuracy is high which means model is not good"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51559e608a57bfe2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c441bde810cd9a9a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
