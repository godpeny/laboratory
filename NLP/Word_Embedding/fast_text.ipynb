{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d71b2ca57192a1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5cc2ce1e8001303"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use existing data\n",
    "# load xml file with tree structure and parse text\n",
    "data_path = \"../data/\"\n",
    "xml = open(data_path+\"ted_en-20160408.xml\", \"r\", encoding=\"UTF8\")\n",
    "xml = etree.parse(xml)\n",
    "parse_text = '\\n'.join(xml.xpath('//content/text()'))\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text) # remove texts in parentheses"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "nltk.download('punkt') # download punkt tokenizer\n",
    "# sentence tokenization : sent_text is list of sentences\n",
    "sent_text = sent_tokenize(content_text)\n",
    "# normalize sentences : remove all except alphabet and number\n",
    "sent_text_normalized = []\n",
    "for s in sent_text:\n",
    "    token = re.sub(\"[^a-zA-Z0-9]\", \" \", s.lower())\n",
    "    sent_text_normalized.append(token)\n",
    "\n",
    "print(sent_text_normalized[:3])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "768067c314137351"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273424\n",
      "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']]\n"
     ]
    }
   ],
   "source": [
    "# tokenize words\n",
    "word_tokenized_text = [word_tokenize(s) for s in sent_text_normalized]\n",
    "print(len(word_tokenized_text)) # number of sentences with tokenized words\n",
    "print(word_tokenized_text[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:21:10.948738Z",
     "start_time": "2023-10-28T10:20:59.071723Z"
    }
   },
   "id": "99b3693710a7211a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fc7e28f1d44b6b8"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "model = FastText(sentences=word_tokenized_text, window=5, min_count=5, workers=4, sg=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:23:51.034128Z",
     "start_time": "2023-10-28T10:23:19.170436Z"
    }
   },
   "id": "f4c8ece4cceca8f9"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[('electrolux', 0.8682365417480469),\n ('electrolyte', 0.8672255873680115),\n ('electroshock', 0.8495044112205505),\n ('electro', 0.847086489200592),\n ('electrochemical', 0.8444557785987854),\n ('airbus', 0.8321382403373718),\n ('electroencephalogram', 0.8314631581306458),\n ('airbag', 0.8235702514648438),\n ('electrogram', 0.817474901676178),\n ('electromagnet', 0.8151991963386536)]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "model.wv.most_similar(\"electrofishing\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:24:35.758084Z",
     "start_time": "2023-10-28T10:24:35.697444Z"
    }
   },
   "id": "1df15ac9ca4236f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "643d2e2bfdc73f22"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
