{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# https://keras.io/examples/nlp/lstm_seq2seq/\n",
    "# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc873cd3c246156d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:25:33.743931Z",
     "start_time": "2023-12-03T10:25:31.354490Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    },
    {
     "data": {
      "text/plain": "                          src                               tar\n59990  That's to be expected.             Il faut s'y attendre.\n59991  That's totally normal.          C'est totalement normal.\n59992  That's totally normal.        C'est parfaitement normal.\n59993  That's totally normal.         C'est tout à fait normal.\n59994  That's totally untrue.          C'est complètement faux.\n59995  That's true, isn't it?        C'est vrai, n'est-ce pas ?\n59996  That's useful to know.               C'est bon à savoir.\n59997  That's very dangerous.             C'est très dangereux.\n59998  That's very dishonest.            C'est très malhonnête.\n59999  That's very good news.  Ce sont d'excellentes nouvelles.",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>src</th>\n      <th>tar</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>59990</th>\n      <td>That's to be expected.</td>\n      <td>Il faut s'y attendre.</td>\n    </tr>\n    <tr>\n      <th>59991</th>\n      <td>That's totally normal.</td>\n      <td>C'est totalement normal.</td>\n    </tr>\n    <tr>\n      <th>59992</th>\n      <td>That's totally normal.</td>\n      <td>C'est parfaitement normal.</td>\n    </tr>\n    <tr>\n      <th>59993</th>\n      <td>That's totally normal.</td>\n      <td>C'est tout à fait normal.</td>\n    </tr>\n    <tr>\n      <th>59994</th>\n      <td>That's totally untrue.</td>\n      <td>C'est complètement faux.</td>\n    </tr>\n    <tr>\n      <th>59995</th>\n      <td>That's true, isn't it?</td>\n      <td>C'est vrai, n'est-ce pas ?</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>That's useful to know.</td>\n      <td>C'est bon à savoir.</td>\n    </tr>\n    <tr>\n      <th>59997</th>\n      <td>That's very dangerous.</td>\n      <td>C'est très dangereux.</td>\n    </tr>\n    <tr>\n      <th>59998</th>\n      <td>That's very dishonest.</td>\n      <td>C'est très malhonnête.</td>\n    </tr>\n    <tr>\n      <th>59999</th>\n      <td>That's very good news.</td>\n      <td>Ce sont d'excellentes nouvelles.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../data/\"\n",
    "\n",
    "df = pd.read_csv(data_path + \"fra.txt\", sep=\"\\t\", names=[\"src\", \"tar\", \"lic\"])\n",
    "del df['lic'] # remove license column which is not needed\n",
    "df = df[0:60000] # use only 60,000 samples for \n",
    "\n",
    "print(len(df))\n",
    "df.tail(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:25:34.229191Z",
     "start_time": "2023-12-03T10:25:33.753135Z"
    }
   },
   "id": "29fbc107d2c1b8c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e36d347e1eae6f5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                          src                                     tar\n57811  I was right all along.   \\t J'avais raison depuis le début. \\n\n18960        You're immature.               \\t Vous êtes immature. \\n\n6977            You're early.                    \\t Tu es matinal. \\n\n38983    Are you enjoying it?                \\t Cela te plaît-il ? \\n\n55988  He seems to be honest.                \\t Il semble honnête. \\n\n59811  That car is very fast.    \\t Cette voiture est très rapide. \\n\n29210      The boat capsized.                \\t Le bateau chavira. \\n\n15201        I called Tom up.  \\t J'ai passé un coup de fil à Tom. \\n\n23688       Tom was a farmer.                \\t Tom était fermier. \\n\n28706      Prices are rising.          \\t Les prix sont en hausse. \\n",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>src</th>\n      <th>tar</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>57811</th>\n      <td>I was right all along.</td>\n      <td>\\t J'avais raison depuis le début. \\n</td>\n    </tr>\n    <tr>\n      <th>18960</th>\n      <td>You're immature.</td>\n      <td>\\t Vous êtes immature. \\n</td>\n    </tr>\n    <tr>\n      <th>6977</th>\n      <td>You're early.</td>\n      <td>\\t Tu es matinal. \\n</td>\n    </tr>\n    <tr>\n      <th>38983</th>\n      <td>Are you enjoying it?</td>\n      <td>\\t Cela te plaît-il ? \\n</td>\n    </tr>\n    <tr>\n      <th>55988</th>\n      <td>He seems to be honest.</td>\n      <td>\\t Il semble honnête. \\n</td>\n    </tr>\n    <tr>\n      <th>59811</th>\n      <td>That car is very fast.</td>\n      <td>\\t Cette voiture est très rapide. \\n</td>\n    </tr>\n    <tr>\n      <th>29210</th>\n      <td>The boat capsized.</td>\n      <td>\\t Le bateau chavira. \\n</td>\n    </tr>\n    <tr>\n      <th>15201</th>\n      <td>I called Tom up.</td>\n      <td>\\t J'ai passé un coup de fil à Tom. \\n</td>\n    </tr>\n    <tr>\n      <th>23688</th>\n      <td>Tom was a farmer.</td>\n      <td>\\t Tom était fermier. \\n</td>\n    </tr>\n    <tr>\n      <th>28706</th>\n      <td>Prices are rising.</td>\n      <td>\\t Les prix sont en hausse. \\n</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"\\t\" is start of sequence(<sos>) and \"\\n\" is end of sequence(<eos>)\n",
    "df.tar = df.tar.apply(lambda x: \"\\t \" + x + \" \\n\") \n",
    "df.sample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:25:34.244617Z",
     "start_time": "2023-12-03T10:25:34.229437Z"
    }
   },
   "id": "c87f6c2e26401c98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizing\n",
    "## tokenizing for each char"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9060d7dad1b9df8d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# get chars\n",
    "src_char = set() # english char\n",
    "tar_char = set() # french char\n",
    "\n",
    "for line in df.src:\n",
    "    for char in line:\n",
    "        src_char.add(char)\n",
    "        \n",
    "for line in df.tar:\n",
    "    for char in line:\n",
    "        tar_char.add(char)\n",
    "\n",
    "# sort chars\n",
    "src_char_sorted = sorted(src_char)\n",
    "tar_char_sorted = sorted(tar_char)    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:25:34.365004Z",
     "start_time": "2023-12-03T10:25:34.260215Z"
    }
   },
   "id": "25c1a14421ac6a43"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 104\n",
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x']\n"
     ]
    }
   ],
   "source": [
    "src_max = len(src_char_sorted) + 1\n",
    "tar_max = len(tar_char_sorted) + 1\n",
    "\n",
    "print(src_max, tar_max)\n",
    "print(src_char_sorted[45:75])\n",
    "print(tar_char_sorted[45:75])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:25:34.369368Z",
     "start_time": "2023-12-03T10:25:34.366432Z"
    }
   },
   "id": "c9d37c5d6f0d968b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, 'ï': 77, '’': 78, '€': 79}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77, '\\xa0': 78, '«': 79, '»': 80, 'À': 81, 'Ç': 82, 'É': 83, 'Ê': 84, 'Ô': 85, 'à': 86, 'â': 87, 'ç': 88, 'è': 89, 'é': 90, 'ê': 91, 'ë': 92, 'î': 93, 'ï': 94, 'ô': 95, 'ù': 96, 'û': 97, 'œ': 98, '\\u2009': 99, '‘': 100, '’': 101, '\\u202f': 102, '‽': 103}\n"
     ]
    }
   ],
   "source": [
    "# char to index\n",
    "src_index = dict([(char, i+1) for i, char in enumerate(src_char_sorted)])\n",
    "tar_index = dict([(char, i+1) for i, char in enumerate(tar_char_sorted)])\n",
    "\n",
    "print(src_index)\n",
    "print(tar_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:25:34.371173Z",
     "start_time": "2023-12-03T10:25:34.369462Z"
    }
   },
   "id": "b1d37e87ee592cc0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 10], [30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10]]\n",
      "[[1, 3, 48, 52, 3, 4, 3, 2], [1, 3, 39, 52, 69, 54, 59, 56, 14, 3, 2], [1, 3, 31, 65, 3, 69, 66, 72, 71, 56, 3, 4, 3, 2], [1, 3, 28, 66, 72, 58, 56, 3, 4, 3, 2], [1, 3, 45, 52, 63, 72, 71, 3, 4, 3, 2]]\n",
      "[[3, 48, 52, 3, 4, 3, 2], [3, 39, 52, 69, 54, 59, 56, 14, 3, 2], [3, 31, 65, 3, 69, 66, 72, 71, 56, 3, 4, 3, 2], [3, 28, 66, 72, 58, 56, 3, 4, 3, 2], [3, 45, 52, 63, 72, 71, 3, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# encoding src and tar for encoder and decoder\n",
    "encoder_input = []\n",
    "decoder_input = []\n",
    "decoder_target = []\n",
    "\n",
    "for line in df.src:\n",
    "    encoder_input.append([src_index[char] for char in line])\n",
    "\n",
    "for line in df.tar:\n",
    "    decoder_input.append([tar_index[char] for char in line])\n",
    "    \n",
    "for line in df.tar:\n",
    "    decoder_target.append([tar_index[char] for char in line][1:]) # remove <sos> token \n",
    "    \n",
    "print(encoder_input[:5])\n",
    "print(decoder_input[:5]) # decoder_input has <sos> token which is index 0 and <eos> token which is index 1\n",
    "print(decoder_target[:5]) # <sos> token is removed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:25:34.631713Z",
     "start_time": "2023-12-03T10:25:34.411162Z"
    }
   },
   "id": "2f2c08ecdbf5af7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "# padding with max length of each src and tar\n",
    "max_src_len = max([len(line) for line in encoder_input])\n",
    "max_tar_len = max([len(line) for line in decoder_input])\n",
    "\n",
    "print(max_src_len)\n",
    "print(max_tar_len)\n",
    "\n",
    "encoder_input_pad = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input_pad = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target_pad = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:25:34.863970Z",
     "start_time": "2023-12-03T10:25:34.632229Z"
    }
   },
   "id": "e200a59a6e0736e3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 22, 80) (60000, 76, 104) (60000, 76, 104)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "encoder_input_onehot = to_categorical(encoder_input_pad)\n",
    "decoder_input_onehot = to_categorical(decoder_input_pad)\n",
    "decoder_target_onehot = to_categorical(decoder_target_pad)\n",
    "\n",
    "# (number of sentences, max length of sentence, number of chars index)\n",
    "print(encoder_input_onehot.shape, decoder_input_onehot.shape, decoder_target_onehot.shape) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:25:35.330165Z",
     "start_time": "2023-12-03T10:25:34.849834Z"
    }
   },
   "id": "2b96a20be00fc31f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3281f5f172b2e903"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Teacher Forcing\n",
    "### https://en.wikipedia.org/wiki/Teacher_forcing\n",
    " - A method for quickly and efficiently training recurrent neural network models that use the ground truth from a prior time step as input.\n",
    " - Feeding observed sequence values (i.e. ground-truth samples) back into the RNN after each step, thus forcing the RNN to stay close to the ground-truth sequence.\n",
    " - It is trained to turn the target sequences into the same sequences but offset by one timestep in the future. -> decoder learns to generate targets[t+1...] with given targets[...t].\n",
    " - 'decoder_target' is 'decoder_input' offset by one timestep. Since <sos> token is removed from 'decoder_target', the length of 'decoder_target' is one less than 'decoder_input'.\n",
    " - For example, \"[1, 3, 48, 52, 3, 4, 3, 2] -> [3, 48, 52, 3, 4, 3, 2]\" : (context vector + 1 -> 3), (context vector + 1 + 3 -> 48), (context vector + 1 + 3 + 48 -> 52) ... and so on."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a47ac9a4a5d70683"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:25:35.331790Z",
     "start_time": "2023-12-03T10:25:35.330803Z"
    }
   },
   "id": "22fca03d0318b6a2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None, 80)]           0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None, 104)]          0         []                            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 256),                345088    ['input_1[0][0]']             \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, None, 256),          369664    ['input_2[0][0]',             \n",
      "                              (None, 256),                           'lstm[0][1]',                \n",
      "                              (None, 256)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 104)            26728     ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 741480 (2.83 MB)\n",
      "Trainable params: 741480 (2.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "latent_dim = 256 # LSTM hidden layer size\n",
    "\n",
    "# encoder\n",
    "e_input = Input(shape=(None, src_max))\n",
    "encoder_lstm = LSTM(units=latent_dim, return_state=True)\n",
    "encoder_output, state_h, state_c = encoder_lstm(e_input)# return_state=True : return hidden state and cell state\n",
    "encoder_states = [state_h, state_c] # hidden state and cell state = context vector\n",
    "\n",
    "# decoder\n",
    "d_input = Input(shape=(None, tar_max)) # teacher forcing input\n",
    "decoder_lstm = LSTM(units=latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(d_input, initial_state=encoder_states) # initial_state : last hidden state of the encoder\n",
    "decoder_softmax = Dense(tar_max, activation='softmax')\n",
    "decoder_outputs = decoder_softmax(decoder_outputs)\n",
    "\n",
    "# model\n",
    "model = Model(inputs=[e_input, d_input], outputs=decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:25:35.767975Z",
     "start_time": "2023-12-03T10:25:35.333051Z"
    }
   },
   "id": "51d850fbdd91be35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LSTM \n",
    " - return_state : configuring a RNN layer to return a list where the first entry is the outputs and the next entries are the internal RNN states. This is used to recover the states of the encoder.\n",
    " - initial_state : initial state(s) of a RNN. This is used to pass the encoder states to the decoder as initial states.\n",
    " - return_sequences : configuring a RNN to return its full sequence of outputs (instead of just the last output, which the defaults behavior). This is used in the decoder.\n",
    "\n",
    "### Context Vector\n",
    " - The context vector is the final hidden state of the encoder. (컨텍스트 벡터는 사실 인코더에서의 마지막 RNN 셀의 은닉 상태값을 말하는 것이며, 이는 입력 문장의 모든 단어 토큰들의 정보를 요약해서 담고 있다.)\n",
    " - The encoder hidden state is used as the initial hidden state of the decoder.\n",
    "\n",
    "### Decoder\n",
    " - The decoder is trained to predict the next word in the sequence given the previous word(s) and the context vector.\n",
    " - 디코더의 첫번째 RNN 셀은 이 첫번째 은닉 상태의 값(Context Vector) + 현재 t에서의 입력값인 <sos>로부터 다음에 등장할 단어를 예측.\n",
    " - 예측된 단어는 다음 시점인 t+1 RNN에서의 입력값이 되고, 이 t+1에서의 RNN 또한 이 입력값 + t에서의 은닉 상태(hidden state)로 다음에 등장할 단어를 예측."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a151363104a9a176"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 174s 231ms/step - loss: 0.7347 - val_loss: 0.6287\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[encoder_input_onehot, decoder_input_onehot], y=decoder_target_onehot, batch_size=64, epochs=1, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:28:31.004189Z",
     "start_time": "2023-12-03T10:25:35.771671Z"
    }
   },
   "id": "8fb5b4ce70de0889"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inferencing\n",
    "- not using model from training phase but using trained layers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "128c36d8b9ed91e3"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, None, 104)]          0         []                            \n",
      "                                                                                                  \n",
      " input_13 (InputLayer)       [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)       [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, None, 256),          369664    ['input_2[0][0]',             \n",
      "                              (None, 256),                           'input_13[0][0]',            \n",
      "                              (None, 256)]                           'input_14[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 104)            26728     ['lstm_1[4][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 396392 (1.51 MB)\n",
      "Trainable params: 396392 (1.51 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "inf_encoder = Model(inputs=e_input, outputs=encoder_states)\n",
    "\n",
    "# decoder\n",
    "# tensor for save values of previous time step\n",
    "inf_decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "inf_decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "inf_decoder_state_inputs = [inf_decoder_state_input_h, inf_decoder_state_input_c]\n",
    "# using saved states of previous time step to predict next char (ref. 'decode_sequence' function)\n",
    "# decoder_lstm_2 = LSTM(units=latent_dim, return_sequences=True, return_state=True)\n",
    "inf_decoder_output, inf_state_h, inf_state_c = decoder_lstm(d_input, initial_state=inf_decoder_state_inputs)\n",
    "# softmax layer\n",
    "inf_decoder_states = [inf_state_h, inf_state_c]\n",
    "# decoder_softmax_2 = Dense(tar_max, activation='softmax')\n",
    "inf_decoder_output = decoder_softmax(inf_decoder_output)\n",
    "\n",
    "inf_decoder = Model(inputs=[d_input] + inf_decoder_state_inputs, outputs=[inf_decoder_output] + inf_decoder_states)\n",
    "\n",
    "inf_decoder.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:37:37.213712Z",
     "start_time": "2023-12-03T10:37:37.080246Z"
    }
   },
   "id": "7de151abe1d2a87b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functional API\n",
    " - Functional API is a way to create models that is more flexible than the tf.keras.Sequential API.\n",
    "### Why use existing model layer from training? e.g., 'decoder_lstm', 'decoder_softmax'\n",
    " - The reason is that we need the model layer to have the same weights as the trained model. If define new model layer (decoder_lstm_2, decoder_softmax_2), the weights earned from training will not be used so training and inferencing phase will have different weights. \n",
    "### Encoder\n",
    " - encoder_inputs와 encoder_states는 훈련 과정에서 이미 정의한 것들을 재사용합니다. 이렇게 되면 훈련 단계에 encoder_inputs와 encoder_states 사이에 있는 모든 층까지 전부 불러오게 되므로 결과적으로 훈련 단계에서 사용한 인코더를 그대로 재사용."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b3769f1dcf330e6"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ' ', 2: '!', 3: '\"', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: '?', 24: 'A', 25: 'B', 26: 'C', 27: 'D', 28: 'E', 29: 'F', 30: 'G', 31: 'H', 32: 'I', 33: 'J', 34: 'K', 35: 'L', 36: 'M', 37: 'N', 38: 'O', 39: 'P', 40: 'Q', 41: 'R', 42: 'S', 43: 'T', 44: 'U', 45: 'V', 46: 'W', 47: 'X', 48: 'Y', 49: 'Z', 50: 'a', 51: 'b', 52: 'c', 53: 'd', 54: 'e', 55: 'f', 56: 'g', 57: 'h', 58: 'i', 59: 'j', 60: 'k', 61: 'l', 62: 'm', 63: 'n', 64: 'o', 65: 'p', 66: 'q', 67: 'r', 68: 's', 69: 't', 70: 'u', 71: 'v', 72: 'w', 73: 'x', 74: 'y', 75: 'z', 76: 'é', 77: 'ï', 78: '’', 79: '€'}\n",
      "{1: '\\t', 2: '\\n', 3: ' ', 4: '!', 5: '\"', 6: '$', 7: '%', 8: '&', 9: \"'\", 10: '(', 11: ')', 12: ',', 13: '-', 14: '.', 15: '0', 16: '1', 17: '2', 18: '3', 19: '4', 20: '5', 21: '6', 22: '7', 23: '8', 24: '9', 25: ':', 26: '?', 27: 'A', 28: 'B', 29: 'C', 30: 'D', 31: 'E', 32: 'F', 33: 'G', 34: 'H', 35: 'I', 36: 'J', 37: 'K', 38: 'L', 39: 'M', 40: 'N', 41: 'O', 42: 'P', 43: 'Q', 44: 'R', 45: 'S', 46: 'T', 47: 'U', 48: 'V', 49: 'W', 50: 'X', 51: 'Y', 52: 'a', 53: 'b', 54: 'c', 55: 'd', 56: 'e', 57: 'f', 58: 'g', 59: 'h', 60: 'i', 61: 'j', 62: 'k', 63: 'l', 64: 'm', 65: 'n', 66: 'o', 67: 'p', 68: 'q', 69: 'r', 70: 's', 71: 't', 72: 'u', 73: 'v', 74: 'w', 75: 'x', 76: 'y', 77: 'z', 78: '\\xa0', 79: '«', 80: '»', 81: 'À', 82: 'Ç', 83: 'É', 84: 'Ê', 85: 'Ô', 86: 'à', 87: 'â', 88: 'ç', 89: 'è', 90: 'é', 91: 'ê', 92: 'ë', 93: 'î', 94: 'ï', 95: 'ô', 96: 'ù', 97: 'û', 98: 'œ', 99: '\\u2009', 100: '‘', 101: '’', 102: '\\u202f', 103: '‽'}\n"
     ]
    }
   ],
   "source": [
    "idx_src = dict([(i, char) for char, i in src_index.items()])\n",
    "idx_tar = dict([(i, char) for char, i in tar_index.items()])\n",
    "\n",
    "print(idx_src)\n",
    "print(idx_tar)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:37:38.010028Z",
     "start_time": "2023-12-03T10:37:38.004588Z"
    }
   },
   "id": "5420c9727c0259f1"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = inf_encoder.predict(input_seq, verbose=0)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_max))\n",
    "    target_seq[0, 0, tar_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = inf_decoder.predict([target_seq] + states_value, verbose=0) # <SOS> + context vector\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "                len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_max))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:37:38.532752Z",
     "start_time": "2023-12-03T10:37:38.529568Z"
    }
   },
   "id": "b0fcf0121692d9d0"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장: Bouge ! \n",
      "번역 문장: Tom est en coune de mont ? \n",
      "-----------------------------------\n",
      "입력 문장: Hello!\n",
      "정답 문장: Bonjour ! \n",
      "번역 문장: Tom est an de mon de le mont ? \n",
      "-----------------------------------\n",
      "입력 문장: Got it!\n",
      "정답 문장: J'ai pigé ! \n",
      "번역 문장: La sous en parde ! \n",
      "-----------------------------------\n",
      "입력 문장: Go home.\n",
      "정답 문장: Rentre à la maison. \n",
      "번역 문장: Tom est an de mon de le mont ? \n",
      "-----------------------------------\n",
      "입력 문장: Get going.\n",
      "정답 문장: En avant. \n",
      "번역 문장: La sous de monte de paster. \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input_onehot[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', df.src[seq_index])\n",
    "    print('정답 문장:', df.tar[seq_index][2:len(df.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T10:37:44.182054Z",
     "start_time": "2023-12-03T10:37:39.057433Z"
    }
   },
   "id": "3ec631699d4d7c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
