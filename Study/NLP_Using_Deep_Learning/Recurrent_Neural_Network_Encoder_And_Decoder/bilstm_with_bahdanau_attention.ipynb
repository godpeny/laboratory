{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-11T13:22:07.675506Z",
     "start_time": "2023-12-11T13:22:03.553452Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n",
      "238.71364\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "max_len = 500\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "print(max(map(len, X_train)))\n",
    "print(sum(map(len, X_train))/len(X_train))\n",
    "\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T13:30:15.704903Z",
     "start_time": "2023-12-11T13:30:13.749247Z"
    }
   },
   "id": "f2abb22edccb7a1d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bahdanau Attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e16de44da7d6f7f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.layers import Dense"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T14:32:17.000784Z",
     "start_time": "2023-12-11T14:32:16.992466Z"
    }
   },
   "id": "493fb5422257d94b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class BahdanauAttention(Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "        \n",
    "    def call(self, values, query):\n",
    "        # query shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights : (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T13:41:57.996197Z"
    }
   },
   "id": "2695e65434c040c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### super(BahdanauAttention, self).__init__()\n",
    " - super() 라는 함수는 super class 즉, 부모 클래스의 임시적인 객체를 반환하여 부모클래스의 메소드를 사용할 수 있게 하는 것.\n",
    " - ``super(BahdanauAttention, self)`` : returns a temporary object of the superclass, which in this case is ``tf.keras.Model``, as ``BahdanauAttention`` is a subclass of tf.keras.Model.\n",
    " - ``super(BahdanauAttention, self).__init__()`` :  calls the constructor of the superclass ``(tf.keras.Model)``. This is necessary to ensure that the initialization code in the base class ``(tf.keras.Model)`` is executed.\n",
    " - In summary, the line ``super(BahdanauAttention, self).__init__()`` in the ``BahdanauAttention`` class ensures that the class inherits and initializes all necessary properties and methods from its parent class ``tf.keras.Model``.\n",
    "\n",
    "### call\n",
    " - ``call`` method is called in the ``BahdanauAttention`` class when you call the instance of the ``BahdanauAttention`` class.\n",
    " - By subclassing the `Model` class: in that case, you should define your\n",
    "    layers in `__init__()` and you should implement the model's forward pass\n",
    "    in `call()`. (https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n",
    "\n",
    "### tf.layers.Dense\n",
    " - Dense implements the operation: ``output = activation(dot(input, kernel) + bias)`` , Belows are all attributes of Dense.\n",
    "  - ``activation`` is the element-wise activation function passed as the activation argument.\n",
    "  - ``kernel`` is a weights matrix created by the layer.\n",
    "  - ``bias`` is a bias vector created by the layer (only applicable if use_bias is True). \n",
    " - If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 0 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units). (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?hl=en)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c684a63d3b359c0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8b71c7c2c3e9f407"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
