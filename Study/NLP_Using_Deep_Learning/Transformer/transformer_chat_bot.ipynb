{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:39.220098Z",
     "start_time": "2023-12-23T01:20:36.743764Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823\n",
      "                 Q            A  label\n",
      "0           12시 땡!   하루가 또 가네요.      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.      0\n",
      "Q        0\n",
      "A        0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/\"\n",
    "df = pd.read_csv(data_path + \"chatbot_data.csv\")\n",
    "print(len(df))\n",
    "print(df.head(5))\n",
    "\n",
    "print(df.isnull().sum()) # no null values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:39.239275Z",
     "start_time": "2023-12-23T01:20:39.221208Z"
    }
   },
   "id": "25d4db072dceb260"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "673f31bcca906976"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:39.521182Z",
     "start_time": "2023-12-23T01:20:39.239195Z"
    }
   },
   "id": "4a0d3e9440b747a2"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']\n",
      "['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .']\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for sentence in df['Q']:\n",
    "    updated_sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    updated_and_stripped_sentence = updated_sentence.strip()\n",
    "    questions.append(updated_and_stripped_sentence)\n",
    "    \n",
    "for sentence in df['A']:\n",
    "    updated_sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    updated_and_stripped_sentence = updated_sentence.strip()\n",
    "    answers.append(updated_and_stripped_sentence)\n",
    "\n",
    "print(questions[:5])\n",
    "print(answers[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:39.551026Z",
     "start_time": "2023-12-23T01:20:39.548649Z"
    }
   },
   "id": "a23e6eac46deaa57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    " - the ``r`` before the string starts a raw string literal, which tells Python not to interpret backslashes as escape characters (common in regular expressions).\n",
    " - ``r\" \\1 \"`` : refers to the content of the first capture group in the regex pattern. In this case, it refers to whichever character (either ?, ., !, or ,) was matched.\n",
    "```python\n",
    "sentence = \"Hello, how are you?\"\n",
    "sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "print(sentence) # Hello ,  how are you ?\n",
    "```\n",
    "\n",
    "### strip() : remove the leading and trailing characters\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a3fa40ea0b8ff4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1de72ae8a5ec20e3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8178] [8179] 8180\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13) # target_vocab_size : the maximum size of the vocabulary to create (if None, no maximum)\n",
    "\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2 # +2 for start and end token\n",
    "\n",
    "print(START_TOKEN, END_TOKEN, VOCAB_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:44.851780Z",
     "start_time": "2023-12-23T01:20:39.564434Z"
    }
   },
   "id": "674d3ccbc955452"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "남편이 하나도 안 도와줘\n",
      "[742, 1126, 20, 2673]\n",
      "742 ----> 남편이 \n",
      "1126 ----> 하나도 \n",
      "20 ----> 안 \n",
      "2673 ----> 도와줘\n"
     ]
    }
   ],
   "source": [
    "print(questions[777])\n",
    "tokenized_q = tokenizer.encode(questions[777])\n",
    "print(tokenized_q)\n",
    "\n",
    "for word in tokenized_q:\n",
    "    print('{} ----> {}'.format(word, tokenizer.decode([word])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:44.854185Z",
     "start_time": "2023-12-23T01:20:44.852261Z"
    }
   },
   "id": "e7a3fc7fdea4b246"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    for sentence_q, sentence_a in zip(inputs, outputs):\n",
    "        sentence_q_p = START_TOKEN + tokenizer.encode(sentence_q) + END_TOKEN\n",
    "        sentence_a_p = START_TOKEN + tokenizer.encode(sentence_a) + END_TOKEN\n",
    "        \n",
    "        tokenized_inputs.append(sentence_q_p)\n",
    "        tokenized_outputs.append(sentence_a_p)\n",
    "        \n",
    "    tokenized_inputs_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences=tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences=tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "        \n",
    "    return tokenized_inputs_padded, tokenized_outputs_padded"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:44.859480Z",
     "start_time": "2023-12-23T01:20:44.855337Z"
    }
   },
   "id": "acfea23fbeaa4a13"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11823, 40) (11823, 40)\n",
      "[8178  742 1126   20 2673 8179    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "questions_processed, answers_processed = tokenize_and_filter(questions, answers)\n",
    "\n",
    "print(questions_processed.shape, answers_processed.shape)\n",
    "print(questions_processed[777])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:45.049418Z",
     "start_time": "2023-12-23T01:20:44.900102Z"
    }
   },
   "id": "8ee93e43f8010c42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f4665e5a1c97fd1"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions_processed,\n",
    "        'dec_inputs': answers_processed[:, :-1] # remove the last token\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers_processed[:, 1:] # remove the first token\n",
    "    },\n",
    "))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:45.064086Z",
     "start_time": "2023-12-23T01:20:45.050528Z"
    }
   },
   "id": "135e71317ec8886a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### dataset.cache\n",
    " - The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\n",
    "### dataset.prefetch\n",
    " -  This allows later elements to be prepared while the current element is being processed. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e7dbf5298bb5185"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "209297951b2eaa0b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    \n",
    "    depth : d_model / num_heads\n",
    "    ... : batch_size\n",
    "    \n",
    "    Args:\n",
    "        query: query shape == (..., num_heads, seq_len_q, depth)\n",
    "        key: key shape == (..., num_heads, seq_len_k, depth)\n",
    "        value: value shape == (..., num_heads, seq_len_v, depth)     \n",
    "        mask : mask shape == (..., 1, 1, seq_len_k)   \n",
    "        \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(a=query, b=key, transpose_b=True)  # Q*K while K is transposed. (..., num_heads, seq_len_q, seq_len_k)\n",
    "    depth_float = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    attention_logits = matmul_qk / tf.math.sqrt(depth_float)  # scale matmul_qk\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        attention_logits += (mask * -1e9)  # -1e9 : -infinite\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    # calculate the attention weights(== attention distribution).\n",
    "    attention_weights = tf.nn.softmax(attention_logits, axis=-1)  # (..., num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    attention_values = tf.matmul(attention_weights, value)  # (..., num_heads, seq_len_q, depth)\n",
    "\n",
    "    return attention_values, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:45.069261Z",
     "start_time": "2023-12-23T01:20:45.064220Z"
    }
   },
   "id": "fa8f280f72caf65e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        # make sure d_model can be divided by num_heads\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads # // : floor division\n",
    "\n",
    "        # WQ, WK, WV\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        # WO\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        \"\"\"Split Query, Key, Value with num_heads\n",
    "\n",
    "        Args:\n",
    "            inputs: input shape == (batch_size, seq_len, d_model)\n",
    "            batch_size: batch size\n",
    "\n",
    "        Returns:\n",
    "            result: result shape == (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        inputs = tf.reshape(tensor=inputs, shape=(batch_size, -1, self.num_heads, self.depth)) # (batch_size, seq_len, num_heads, depth)\n",
    "        return tf.transpose(a=inputs, perm=[0, 2, 1, 3]) # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"MultiHeadAttention\n",
    "\n",
    "        Args:\n",
    "            inputs: Q, K, V, mask\n",
    "                Q shape == (batch_size, seq_len_q, d_model)\n",
    "                K shape == (batch_size, seq_len_k, d_model)\n",
    "                V shape == (batch_size, seq_len_v, d_model)\n",
    "                mask shape == (batch_size, seq_len_q, seq_len_k)\n",
    "\n",
    "        Returns:\n",
    "            output, attention_weights\n",
    "        \"\"\"\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(input=query)[0]\n",
    "\n",
    "        # 1. Q,K,V linear layer\n",
    "        query = self.query_dense(query) # (batch_size, seq_len_q, d_model)\n",
    "        key = self.key_dense(key) # (batch_size, seq_len_k, d_model)\n",
    "        value = self.value_dense(value) # (batch_size, seq_len_v, d_model)\n",
    "\n",
    "        # 2. split heads\n",
    "        query = self.split_heads(query, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        key = self.split_heads(key, batch_size) # (batch_size, num_heads, seq_len_k, depth)\n",
    "        value = self.split_heads(value, batch_size) # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # 3. scaled dot-product attention\n",
    "        temp_attention_values, _ = scaled_dot_product_attention(query, key, value, mask) # (batch_size, num_heads, seq_len_q, depth)\n",
    "\n",
    "        # 4. transpose result and concat heads\n",
    "        temp_attention_values = tf.transpose(a=temp_attention_values, perm=[0, 2, 1, 3]) # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_temp_attention_values = tf.reshape(tensor=temp_attention_values, shape=(batch_size, -1, self.d_model)) # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        # 5. final linear layer\n",
    "        attention_values = self.dense(concat_temp_attention_values) # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return attention_values\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:45.087466Z",
     "start_time": "2023-12-23T01:20:45.070200Z"
    }
   },
   "id": "1cca60fce0159cfb"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position = tf.range(position, dtype=tf.float32)[:, tf.newaxis], # position -> (position, 1)\n",
    "            i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], # d_model -> (1, d_model)\n",
    "            d_model = d_model\n",
    "        )\n",
    "\n",
    "        # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin 과 cos 를 붙이기\n",
    "        zeros = np.zeros(angle_rads.shape)\n",
    "        zeros[:, 0::2] = sines\n",
    "        zeros[:, 1::2] = cosines\n",
    "\n",
    "        pos_encoding = tf.constant(zeros) # [[s,c,s,c,..s,c]]\n",
    "\n",
    "        # pos_encoding 은 (1, position, d_model) 의 shape 을 가짐\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:45.087762Z",
     "start_time": "2023-12-23T01:20:45.074465Z"
    }
   },
   "id": "1d2718c49438ed5e"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout_ratio, name=\"encoder_layer\"):\n",
    "    \"\"\"Encoder layer\n",
    "\n",
    "    Args:\n",
    "        dff: hidden layer size\n",
    "        d_model: embedding size\n",
    "        num_heads: number of heads\n",
    "        dropout_ratio: dropout ratio\n",
    "        name: encoder layer name\n",
    "\n",
    "    Returns:\n",
    "        output: output of encoder layer\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 1-1. Multi-Head Attention\n",
    "    attention_values = MultiHeadAttention(d_model, num_heads, name=\"attention\")(inputs={\n",
    "        'query': inputs,\n",
    "        'key': inputs,\n",
    "        'value': inputs,\n",
    "        'mask': padding_mask}) # Q=K=V\n",
    "\n",
    "    # 1-2. Dropout + Residual Connection + Layer Normalization\n",
    "    attention_values = tf.keras.layers.Dropout(rate=dropout_ratio)(attention_values)\n",
    "    # Residual Connection : inputs + attention_values\n",
    "    # epsilon : a small number to avoid zero division\n",
    "    attention_values = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention_values)\n",
    "\n",
    "    # 2. Position-Wise Feed Forward Neural Networks (fully connected FFNN)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention_values)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 2-2. Dropout + Residual Connection + Layer Normalization\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout_ratio)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_values + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:45.088147Z",
     "start_time": "2023-12-23T01:20:45.077898Z"
    }
   },
   "id": "9c1915acadd25ceb"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def encoder(vocab_size, num_layers, dff, d_model, num_heads, dropout_ratio, name=\"encoder\"):\n",
    "    \"\"\"Encoder\n",
    "\n",
    "    Args:\n",
    "        vocab_size: vocab size\n",
    "        num_layers: number of layers\n",
    "        dff: hidden layer size\n",
    "        d_model: embedding size\n",
    "        num_heads: number of heads\n",
    "        dropout_ratio: dropout ratio\n",
    "        name: encoder name\n",
    "\n",
    "    Returns:\n",
    "        output: output of encoder\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 1. Embedding\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) # scale. # 그래디언트 배니싱 문제를 완화하는 테크닉 (https://nlp.seas.harvard.edu/2018/04/03/attention.html, 'Embeddings and Softmax' 참고)\n",
    "\n",
    "    # 2. Positional Encoding + Dropout\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout_ratio)(embeddings)\n",
    "\n",
    "    # 3. Stacking Encoder Layers by num_layers\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout_ratio=dropout_ratio, name=\"encoder_layer_{}\".format(i),)(inputs=[outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:45.088580Z",
     "start_time": "2023-12-23T01:20:45.083218Z"
    }
   },
   "id": "ff85be5404ee0fc"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def create_mask(x):\n",
    "    \"\"\"Create mask for padding\n",
    "    \n",
    "    Args:\n",
    "        x: input sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask: mask for padding\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32) # 0 is padding value and find it.\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "    \"\"\"Create mask for look ahead\n",
    "    \n",
    "    Args:\n",
    "        x: input sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask: mask for look ahead\n",
    "    \"\"\"\n",
    "    seq_len = tf.shape(input=x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) # lower triangle is zero, upper triangle is one.\n",
    "    padding_mask = create_mask(x) # if value is 0, then mask is 1.\n",
    "    return tf.maximum(look_ahead_mask, padding_mask) # if 1 is set on any of both masks, then final mask is 1."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:45.088698Z",
     "start_time": "2023-12-23T01:20:45.085958Z"
    }
   },
   "id": "eedecd9b22f1691f"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def decoder_layer(dff, d_model, num_heads, dropout_ratio, name=\"decoder_layer\"):\n",
    "    \"\"\"Decoder layer\n",
    "    \n",
    "    Args:\n",
    "        dff: hidden layer size\n",
    "        d_model: embedding size\n",
    "        num_heads: number of heads\n",
    "        dropout_ratio: dropout ratio\n",
    "        name: decoder layer name\n",
    "    \n",
    "    Returns:\n",
    "        output: output of decoder layer\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    encoder_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 1-1. Multi-Head Attention (self-attention)\n",
    "    attention1 = MultiHeadAttention(d_model, num_heads, name=\"attention-1\")(inputs={\n",
    "        'query': inputs,\n",
    "        'key': inputs,\n",
    "        'value': inputs,\n",
    "        'mask': look_ahead_mask}) # Q=K=V\n",
    "\n",
    "    # 1-2. Dropout + Residual Connection + Layer Normalization\n",
    "    attention1 = tf.keras.layers.Dropout(rate=dropout_ratio)(attention1)\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 2-1. Multi-Head Attention (encoder-decoder attention)\n",
    "    attention2 = MultiHeadAttention(d_model, num_heads, name=\"attention-2\")(inputs={\n",
    "        'query': attention1,\n",
    "        'key': encoder_outputs,\n",
    "        'value': encoder_outputs,\n",
    "        'mask': padding_mask}) # Q=K=V\n",
    "\n",
    "    # 2-2. Dropout + Residual Connection + Layer Normalization\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout_ratio)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 3. Position-Wise Feed Forward Neural Networks (fully connected FFNN)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 3-2. Dropout + Residual Connection + Layer Normalization\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout_ratio)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, encoder_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:45.097157Z",
     "start_time": "2023-12-23T01:20:45.090137Z"
    }
   },
   "id": "1b4280c3f88991c6"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, dff, d_model, num_heads, dropout_ratio, name='decoder'):\n",
    "    \"\"\"Decoder\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: vocab size\n",
    "        num_layers: number of layers\n",
    "        dff: hidden layer size\n",
    "        d_model: embedding size\n",
    "        num_heads: number of heads\n",
    "        dropout_ratio: dropout ratio\n",
    "        name: decoder name\n",
    "    \n",
    "    Returns:\n",
    "        output: output of decoder\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    encoder_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 1. Embedding\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) # scale. 그래디언트 배니싱 문제를 완화하는 테크닉 (https://nlp.seas.harvard.edu/2018/04/03/attention.html, 'Embeddings and Softmax' 참고)\n",
    "\n",
    "    # 2. Positional Encoding + Dropout\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout_ratio)(embeddings)\n",
    "\n",
    "    # 3. Stacking Decoder Layers by num_layers\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout_ratio=dropout_ratio, name='decoder_layer_{}'.format(i),)(inputs=[outputs, encoder_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, encoder_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:45.097595Z",
     "start_time": "2023-12-23T01:20:45.094007Z"
    }
   },
   "id": "1a5d4ef1e9f6f99f"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, dff, d_model, num_heads, dropout_ratio, name=\"transformer\"):\n",
    "    \"\"\"Transformer\n",
    "\n",
    "    Args:\n",
    "        vocab_size: vocab size\n",
    "        num_layers: number of layers\n",
    "        dff: hidden layer size\n",
    "        d_model: embedding size\n",
    "        num_heads: number of heads\n",
    "        dropout_ratio: dropout ratio\n",
    "        name: transformer name\n",
    "\n",
    "    Returns:\n",
    "        output: output of transformer\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 1. Encoder padding mask\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(create_mask, output_shape=(1, 1, None), name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 2. Decoder padding mask\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(create_mask, output_shape=(1, 1, None), name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 3. Look ahead mask\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(create_look_ahead_mask, output_shape=(1, None, None), name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 4. Encoder\n",
    "    enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout_ratio=dropout_ratio,)(inputs=[inputs, enc_padding_mask]) # inputs, padding_mask\n",
    "\n",
    "    # 5. Decoder\n",
    "    dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout_ratio=dropout_ratio,)(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask]) # inputs, encoder_outputs, look_ahead_mask, padding_mask\n",
    "\n",
    "    # 6. Fully Connected Layer\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:20:45.103395Z",
     "start_time": "2023-12-23T01:20:45.097812Z"
    }
   },
   "id": "2d901f954ae7b198"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dff=DFF,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout_ratio=DROPOUT)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:22:26.524669Z",
     "start_time": "2023-12-23T01:22:25.183102Z"
    }
   },
   "id": "5b1914110fd0f0da"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    \"\"\"Loss function\n",
    "\n",
    "    Args:\n",
    "        y_true: true label\n",
    "        y_pred: predicted label\n",
    "\n",
    "    Returns:\n",
    "        loss: cross entropy loss\n",
    "    \"\"\"\n",
    "    y_true = tf.reshape(tensor=y_true, shape=(-1, MAX_LENGTH - 1)) # (batch_size, seq_len - 1)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(y_true, y_pred) # reduction is none because using custom loss. \n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32) # 0 is padding value and find it. (batch_size, seq_len - 1)\n",
    "    loss = tf.multiply(loss, mask) # \n",
    "    return tf.reduce_mean(input_tensor=loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:23:17.453696Z",
     "start_time": "2023-12-23T01:23:17.444128Z"
    }
   },
   "id": "3ac82ebee5566ce5"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:23:18.292592Z",
     "start_time": "2023-12-23T01:23:18.266052Z"
    }
   },
   "id": "7a6e51283407bee2"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # 레이블의 크기는 (batch_size, MAX_LENGTH - 1)\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:27:14.428138Z",
     "start_time": "2023-12-23T01:27:14.356117Z"
    }
   },
   "id": "6ce7b211aa10950"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "185/185 [==============================] - 49s 265ms/step - loss: 0.8229 - accuracy: 0.0607\n",
      "Epoch 2/5\n",
      "185/185 [==============================] - 48s 259ms/step - loss: 0.7594 - accuracy: 0.0661\n",
      "Epoch 3/5\n",
      "185/185 [==============================] - 48s 259ms/step - loss: 0.6885 - accuracy: 0.0734\n",
      "Epoch 4/5\n",
      "185/185 [==============================] - 49s 263ms/step - loss: 0.6122 - accuracy: 0.0818\n",
      "Epoch 5/5\n",
      "185/185 [==============================] - 48s 258ms/step - loss: 0.5299 - accuracy: 0.0911\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:36:50.889967Z",
     "start_time": "2023-12-23T01:32:49.537099Z"
    }
   },
   "id": "3d50729a029e9852"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4272fe7a6f870a6"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 단어와 구두점 사이에 공백 추가.\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "def evaluate(sentence):\n",
    "    # 입력 문장에 대한 전처리\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 입력 문장에 시작 토큰과 종료 토큰을 추가\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 예측 시작\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model(inputs=[sentence, output], training=False) # input is sentence.\n",
    "\n",
    "        # 현재 시점의 예측 단어를 받아온다.\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32) # prediction_id is the index of the word that has the highest probability.\n",
    "\n",
    "        # 만약 현재 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # 현재 시점의 예측 단어를 output(출력)에 연결한다.\n",
    "        # output은 for문의 다음 루프에서 디코더의 입력이 된다.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    # 단어 예측이 모두 끝났다면 output을 리턴.\n",
    "    return tf.squeeze(output, axis=0)\n",
    "\n",
    "def predict(sentence):\n",
    "    prediction = evaluate(sentence)\n",
    "\n",
    "    # prediction == 디코더가 리턴한 챗봇의 대답에 해당하는 정수 시퀀스\n",
    "    # tokenizer.decode()를 통해 정수 시퀀스를 문자열로 디코딩.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:37:40.656557Z",
     "start_time": "2023-12-23T01:37:40.621698Z"
    }
   },
   "id": "b6e6c03ed4dd0cc7"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 영화 볼래?\n",
      "Output: 안 하는 게 좋을 거 같아요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"영화 볼래?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:37:47.530592Z",
     "start_time": "2023-12-23T01:37:47.260525Z"
    }
   },
   "id": "9f755e2536350caf"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 고민이 있어\n",
      "Output: 그런 사람 만나지 마세요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"고민이 있어\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T01:37:55.846293Z",
     "start_time": "2023-12-23T01:37:55.656812Z"
    }
   },
   "id": "85a5de8c7a510fe1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1f97f48e7fa48f63"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
