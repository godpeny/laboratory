{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-17T01:30:51.417963Z",
     "start_time": "2023-12-17T01:30:47.200443Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e373564139411efc"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    \n",
    "    depth : d_model / num_heads\n",
    "    ... : batch_size\n",
    "    \n",
    "    Args:\n",
    "        query: query shape == (..., num_heads, seq_len_q, depth)\n",
    "        key: key shape == (..., num_heads, seq_len_k, depth)\n",
    "        value: value shape == (..., num_heads, seq_len_v, depth)     \n",
    "        mask : mask shape == (..., 1, 1, seq_len_k)   \n",
    "        \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(a=query, b=key, transpose_b=True)  # Q*K while K is transposed. (..., num_heads, seq_len_q, seq_len_k)\n",
    "    depth_float = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    attention_logits = matmul_qk / tf.math.sqrt(depth_float)  # scale matmul_qk\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        attention_logits += (mask * -1e9)  # -1e9 : -infinite\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    # calculate the attention weights(== attention distribution).\n",
    "    attention_weights = tf.nn.softmax(attention_logits, axis=-1)  # (..., num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    attention_values = tf.matmul(attention_weights, value)  # (..., num_heads, seq_len_q, depth)\n",
    "\n",
    "    return attention_values, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T01:30:55.510676Z",
     "start_time": "2023-12-17T01:30:55.508585Z"
    }
   },
   "id": "ba1e7c3e2b7c39d0"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        # make sure d_model can be divided by num_heads\n",
    "        assert d_model % self.num_heads == 0 \n",
    "        self.depth = d_model // self.num_heads # // : floor division\n",
    "        \n",
    "        # WQ, WK, WV\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        # WO\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "    \n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        \"\"\"Split Query, Key, Value with num_heads\n",
    "        \n",
    "        Args:\n",
    "            inputs: input shape == (batch_size, seq_len, d_model)\n",
    "            batch_size: batch size\n",
    "        \n",
    "        Returns:\n",
    "            result: result shape == (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        inputs = tf.reshape(tensor=inputs, shape=(batch_size, -1, self.num_heads, self.depth)) # (batch_size, seq_len, num_heads, depth)\n",
    "        return tf.transpose(a=inputs, perm=[0, 2, 1, 3]) # (batch_size, num_heads, seq_len, depth)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"MultiHeadAttention\n",
    "        \n",
    "        Args:\n",
    "            inputs: Q, K, V, mask\n",
    "                Q shape == (batch_size, seq_len_q, d_model)\n",
    "                K shape == (batch_size, seq_len_k, d_model)\n",
    "                V shape == (batch_size, seq_len_v, d_model)\n",
    "                mask shape == (batch_size, seq_len_q, seq_len_k)\n",
    "        \n",
    "        Returns:\n",
    "            output, attention_weights\n",
    "        \"\"\"\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(input=query)[0]\n",
    "        \n",
    "        # 1. Q,K,V linear layer\n",
    "        query = self.query_dense(query) # (batch_size, seq_len_q, d_model)\n",
    "        key = self.key_dense(key) # (batch_size, seq_len_k, d_model)\n",
    "        value = self.value_dense(value) # (batch_size, seq_len_v, d_model)\n",
    "        \n",
    "        # 2. split heads\n",
    "        query = self.split_heads(query, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        key = self.split_heads(key, batch_size) # (batch_size, num_heads, seq_len_k, depth)\n",
    "        value = self.split_heads(value, batch_size) # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # 3. scaled dot-product attention\n",
    "        temp_attention_values, attention_weights = scaled_dot_product_attention(query, key, value, mask) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        \n",
    "        # 4. transpose result and concat heads\n",
    "        temp_attention_values = tf.transpose(a=temp_attention_values, perm=[0, 2, 1, 3]) # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_temp_attention_values = tf.reshape(tensor=temp_attention_values, shape=(batch_size, -1, self.d_model)) # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        # 5. final linear layer\n",
    "        attention_values = self.dense(concat_temp_attention_values) # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return attention_values, attention_weights\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T01:30:56.184552Z",
     "start_time": "2023-12-17T01:30:56.166385Z"
    }
   },
   "id": "97ddb1dc361ee55f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def create_mask(x):\n",
    "    \"\"\"Create mask for padding\n",
    "    \n",
    "    Args:\n",
    "        x: input sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask: mask for padding\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32) # 0 is padding value and find it.\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T01:30:57.025696Z",
     "start_time": "2023-12-17T01:30:57.017268Z"
    }
   },
   "id": "ec74ebf60ab3c765"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Positional Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab3b2ac3a603a143"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position = tf.range(position, dtype=tf.float32)[:, tf.newaxis], # position -> (position, 1)\n",
    "            i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], # d_model -> (1, d_model)\n",
    "            d_model = d_model\n",
    "        )\n",
    "\n",
    "        # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin 과 cos 를 붙이기\n",
    "        zeros = np.zeros(angle_rads.shape)\n",
    "        zeros[:, 0::2] = sines\n",
    "        zeros[:, 1::2] = cosines\n",
    "\n",
    "        pos_encoding = tf.constant(zeros) # [[s,c,s,c,..s,c]]\n",
    "\n",
    "        # pos_encoding 은 (1, position, d_model) 의 shape 을 가짐\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        print(pos_encoding.shape)\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(inputs)\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T01:32:26.835421Z",
     "start_time": "2023-12-17T01:32:26.821639Z"
    }
   },
   "id": "54404771fb3c75e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63e40efd5c55afca"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout_ratio, name=\"encoder_layer\"):\n",
    "    \"\"\"Encoder layer\n",
    "    \n",
    "    Args:\n",
    "        dff: hidden layer size\n",
    "        d_model: embedding size\n",
    "        num_heads: number of heads\n",
    "        dropout_ratio: dropout ratio\n",
    "        name: encoder layer name\n",
    "    \n",
    "    Returns:\n",
    "        output: output of encoder layer\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    \n",
    "    # 1-1. Multi-Head Attention\n",
    "    attention_values, _ = MultiHeadAttention(d_model, num_heads)(inputs={\n",
    "        'query': inputs, \n",
    "        'key': inputs, \n",
    "        'value': inputs, \n",
    "        'mask': padding_mask}) # Q=K=V\n",
    "    \n",
    "    # 1-2. Dropout + Residual Connection + Layer Normalization\n",
    "    attention_values = tf.keras.layers.Dropout(rate=dropout_ratio)(attention_values)\n",
    "    # Residual Connection : inputs + attention_values\n",
    "    # epsilon : a small number to avoid zero division\n",
    "    attention_values = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention_values) \n",
    "    \n",
    "    # 2. Position-Wise Feed Forward Neural Networks (fully connected FFNN)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention_values)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    \n",
    "    # 2-2. Dropout + Residual Connection + Layer Normalization\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout_ratio)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_values + outputs)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T01:32:33.586643Z",
     "start_time": "2023-12-17T01:32:33.568345Z"
    }
   },
   "id": "2edfe18139aa067"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def encoder(vocab_size, num_layers, dff, d_model, num_heads, dropout_ratio, name=\"encoder\"):\n",
    "    \"\"\"Encoder\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: vocab size\n",
    "        num_layers: number of layers\n",
    "        dff: hidden layer size\n",
    "        d_model: embedding size\n",
    "        num_heads: number of heads\n",
    "        dropout_ratio: dropout ratio\n",
    "        name: encoder name\n",
    "    \n",
    "    Returns:\n",
    "        output: output of encoder\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    \n",
    "    # 1. Embedding\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) # scale. # 그래디언트 배니싱 문제를 완화하는 테크닉 (from 공식 구현체?)\n",
    "    \n",
    "    # 2. Positional Encoding + Dropout\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout_ratio)(embeddings)\n",
    "    \n",
    "    # 3. Stacking Encoder Layers by num_layers\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout_ratio=dropout_ratio, name=\"encoder_layer_{}\".format(i),)(inputs=[outputs, padding_mask])\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T01:45:18.858965Z",
     "start_time": "2023-12-17T01:45:18.855404Z"
    }
   },
   "id": "9fdce24b005b69a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
