{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-17T04:29:21.099868Z",
     "start_time": "2023-12-17T04:29:16.689765Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e373564139411efc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    \n",
    "    depth : d_model / num_heads\n",
    "    ... : batch_size\n",
    "    \n",
    "    Args:\n",
    "        query: query shape == (..., num_heads, seq_len_q, depth)\n",
    "        key: key shape == (..., num_heads, seq_len_k, depth)\n",
    "        value: value shape == (..., num_heads, seq_len_v, depth)     \n",
    "        mask : mask shape == (..., 1, 1, seq_len_k)   \n",
    "        \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(a=query, b=key, transpose_b=True)  # Q*K while K is transposed. (..., num_heads, seq_len_q, seq_len_k)\n",
    "    depth_float = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    attention_logits = matmul_qk / tf.math.sqrt(depth_float)  # scale matmul_qk\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        attention_logits += (mask * -1e9)  # -1e9 : -infinite\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    # calculate the attention weights(== attention distribution).\n",
    "    attention_weights = tf.nn.softmax(attention_logits, axis=-1)  # (..., num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    attention_values = tf.matmul(attention_weights, value)  # (..., num_heads, seq_len_q, depth)\n",
    "\n",
    "    return attention_values, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T04:29:22.395578Z",
     "start_time": "2023-12-17T04:29:22.391178Z"
    }
   },
   "id": "ba1e7c3e2b7c39d0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        # make sure d_model can be divided by num_heads\n",
    "        assert d_model % self.num_heads == 0 \n",
    "        self.depth = d_model // self.num_heads # // : floor division\n",
    "        \n",
    "        # WQ, WK, WV\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        # WO\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "    \n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        \"\"\"Split Query, Key, Value with num_heads\n",
    "        \n",
    "        Args:\n",
    "            inputs: input shape == (batch_size, seq_len, d_model)\n",
    "            batch_size: batch size\n",
    "        \n",
    "        Returns:\n",
    "            result: result shape == (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        inputs = tf.reshape(tensor=inputs, shape=(batch_size, -1, self.num_heads, self.depth)) # (batch_size, seq_len, num_heads, depth)\n",
    "        return tf.transpose(a=inputs, perm=[0, 2, 1, 3]) # (batch_size, num_heads, seq_len, depth)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"MultiHeadAttention\n",
    "        \n",
    "        Args:\n",
    "            inputs: Q, K, V, mask\n",
    "                Q shape == (batch_size, seq_len_q, d_model)\n",
    "                K shape == (batch_size, seq_len_k, d_model)\n",
    "                V shape == (batch_size, seq_len_v, d_model)\n",
    "                mask shape == (batch_size, seq_len_q, seq_len_k)\n",
    "        \n",
    "        Returns:\n",
    "            output, attention_weights\n",
    "        \"\"\"\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(input=query)[0]\n",
    "        \n",
    "        # 1. Q,K,V linear layer\n",
    "        query = self.query_dense(query) # (batch_size, seq_len_q, d_model)\n",
    "        key = self.key_dense(key) # (batch_size, seq_len_k, d_model)\n",
    "        value = self.value_dense(value) # (batch_size, seq_len_v, d_model)\n",
    "        \n",
    "        # 2. split heads\n",
    "        query = self.split_heads(query, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        key = self.split_heads(key, batch_size) # (batch_size, num_heads, seq_len_k, depth)\n",
    "        value = self.split_heads(value, batch_size) # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # 3. scaled dot-product attention\n",
    "        temp_attention_values, _ = scaled_dot_product_attention(query, key, value, mask) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        \n",
    "        # 4. transpose result and concat heads\n",
    "        temp_attention_values = tf.transpose(a=temp_attention_values, perm=[0, 2, 1, 3]) # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_temp_attention_values = tf.reshape(tensor=temp_attention_values, shape=(batch_size, -1, self.d_model)) # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        # 5. final linear layer\n",
    "        attention_values = self.dense(concat_temp_attention_values) # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return attention_values\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T04:29:23.197649Z",
     "start_time": "2023-12-17T04:29:23.178430Z"
    }
   },
   "id": "97ddb1dc361ee55f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def create_mask(x):\n",
    "    \"\"\"Create mask for padding\n",
    "    \n",
    "    Args:\n",
    "        x: input sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask: mask for padding\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32) # 0 is padding value and find it.\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T04:29:23.843087Z",
     "start_time": "2023-12-17T04:29:23.837183Z"
    }
   },
   "id": "ec74ebf60ab3c765"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Positional Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab3b2ac3a603a143"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position = tf.range(position, dtype=tf.float32)[:, tf.newaxis], # position -> (position, 1)\n",
    "            i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], # d_model -> (1, d_model)\n",
    "            d_model = d_model\n",
    "        )\n",
    "\n",
    "        # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin 과 cos 를 붙이기\n",
    "        zeros = np.zeros(angle_rads.shape)\n",
    "        zeros[:, 0::2] = sines\n",
    "        zeros[:, 1::2] = cosines\n",
    "\n",
    "        pos_encoding = tf.constant(zeros) # [[s,c,s,c,..s,c]]\n",
    "\n",
    "        # pos_encoding 은 (1, position, d_model) 의 shape 을 가짐\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        print(pos_encoding.shape)\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(inputs)\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T04:29:25.151499Z",
     "start_time": "2023-12-17T04:29:25.147724Z"
    }
   },
   "id": "54404771fb3c75e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63e40efd5c55afca"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout_ratio, name=\"encoder_layer\"):\n",
    "    \"\"\"Encoder layer\n",
    "    \n",
    "    Args:\n",
    "        dff: hidden layer size\n",
    "        d_model: embedding size\n",
    "        num_heads: number of heads\n",
    "        dropout_ratio: dropout ratio\n",
    "        name: encoder layer name\n",
    "    \n",
    "    Returns:\n",
    "        output: output of encoder layer\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    \n",
    "    # 1-1. Multi-Head Attention\n",
    "    attention_values, _ = MultiHeadAttention(d_model, num_heads)(inputs={\n",
    "        'query': inputs, \n",
    "        'key': inputs, \n",
    "        'value': inputs, \n",
    "        'mask': padding_mask}) # Q=K=V\n",
    "    \n",
    "    # 1-2. Dropout + Residual Connection + Layer Normalization\n",
    "    attention_values = tf.keras.layers.Dropout(rate=dropout_ratio)(attention_values)\n",
    "    # Residual Connection : inputs + attention_values\n",
    "    # epsilon : a small number to avoid zero division\n",
    "    attention_values = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention_values) \n",
    "    \n",
    "    # 2. Position-Wise Feed Forward Neural Networks (fully connected FFNN)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention_values)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    \n",
    "    # 2-2. Dropout + Residual Connection + Layer Normalization\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout_ratio)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_values + outputs)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T04:29:26.394050Z",
     "start_time": "2023-12-17T04:29:26.378375Z"
    }
   },
   "id": "2edfe18139aa067"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def encoder(vocab_size, num_layers, dff, d_model, num_heads, dropout_ratio, name=\"encoder\"):\n",
    "    \"\"\"Encoder\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: vocab size\n",
    "        num_layers: number of layers\n",
    "        dff: hidden layer size\n",
    "        d_model: embedding size\n",
    "        num_heads: number of heads\n",
    "        dropout_ratio: dropout ratio\n",
    "        name: encoder name\n",
    "    \n",
    "    Returns:\n",
    "        output: output of encoder\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    \n",
    "    # 1. Embedding\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) # scale. # 그래디언트 배니싱 문제를 완화하는 테크닉 (from 공식 구현체?)\n",
    "    \n",
    "    # 2. Positional Encoding + Dropout\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout_ratio)(embeddings)\n",
    "    \n",
    "    # 3. Stacking Encoder Layers by num_layers\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout_ratio=dropout_ratio, name=\"encoder_layer_{}\".format(i),)(inputs=[outputs, padding_mask])\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T04:29:27.059586Z",
     "start_time": "2023-12-17T04:29:27.054644Z"
    }
   },
   "id": "9fdce24b005b69a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Masking"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acd950f9f33e903f"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def create_mask(x):\n",
    "    \"\"\"Create mask for padding\n",
    "    \n",
    "    Args:\n",
    "        x: input sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask: mask for padding\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32) # 0 is padding value and find it.\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T04:29:29.785694Z",
     "start_time": "2023-12-17T04:29:29.780405Z"
    }
   },
   "id": "7edcd4d6b3aae607"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    \"\"\"Create mask for look ahead\n",
    "    \n",
    "    Args:\n",
    "        x: input sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask: mask for look ahead\n",
    "    \"\"\"\n",
    "    seq_len = tf.shape(input=x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) # lower triangle is zero, upper triangle is one.\n",
    "    padding_mask = create_mask(x) # if value is 0, then mask is 1.\n",
    "    return tf.maximum(look_ahead_mask, padding_mask) # if 1 is set on any of both masks, then final mask is 1."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T04:29:30.577967Z",
     "start_time": "2023-12-17T04:29:30.557134Z"
    }
   },
   "id": "b4865f6a23c2a62d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tf.linalg.band_part\n",
    " - ``tf.linalg.band_part(input, num_lower, num_upper, name=None)`` : Returns a copy of tensor setting everything outside a central band in each innermost matrix to zero.\n",
    " - ``num_lower`` : 0 is the main diagonal, a positive value is below it. if negative, save all lower triangle.\n",
    " - ``num_upper`` : 0 is the main diagonal, a positive value is above it. if negative, save all upper triangle.\n",
    "```python\n",
    "input = tf.constant([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10], [10, 11, 12, 13]])\n",
    "print(tf.linalg.band_part(input, 2, 1)) \n",
    "\"\"\"\n",
    "[[ 1  2  0  0]\n",
    " [ 4  5  6  0]\n",
    " [ 7  8  9 10]\n",
    " [ 0 11 12 13]]\n",
    "\"\"\"\n",
    "print(tf.linalg.band_part(input, -1, 0))\n",
    "\"\"\"\n",
    "[[ 1  0  0  0]\n",
    " [ 4  5  0  0]\n",
    " [ 7  8  9  0]\n",
    " [10 11 12 13]]\n",
    "\"\"\"\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4566ea968e942902"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 1. 0. 1.]\n",
      "   [0. 0. 1. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_look_ahead_mask(tf.constant([[1, 2, 0, 4, 5]])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T04:41:37.329973Z",
     "start_time": "2023-12-17T04:41:37.305412Z"
    }
   },
   "id": "dbd4b25bd5e7ef2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "504444783d24bcaf"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def decoder_layer(dff, d_model, num_heads, dropout_ratio, name=\"decoder_layer\"):\n",
    "    \"\"\"Decoder layer\n",
    "    \n",
    "    Args:\n",
    "        dff: hidden layer size\n",
    "        d_model: embedding size\n",
    "        num_heads: number of heads\n",
    "        dropout_ratio: dropout ratio\n",
    "        name: decoder layer name\n",
    "    \n",
    "    Returns:\n",
    "        output: output of decoder layer\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    encoder_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    # 1-1. Multi-Head Attention (self-attention)\n",
    "    attention1, _ = MultiHeadAttention(d_model, num_heads)(inputs={\n",
    "        'query': inputs, \n",
    "        'key': inputs, \n",
    "        'value': inputs, \n",
    "        'mask': look_ahead_mask}) # Q=K=V\n",
    "    \n",
    "    # 1-2. Dropout + Residual Connection + Layer Normalization\n",
    "    attention1 = tf.keras.layers.Dropout(rate=dropout_ratio)(attention1)\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "    \n",
    "    # 2-1. Multi-Head Attention (encoder-decoder attention)\n",
    "    attention2 = MultiHeadAttention(d_model, num_heads)(inputs={\n",
    "        'query': attention1, \n",
    "        'key': encoder_outputs, \n",
    "        'value': encoder_outputs, \n",
    "        'mask': padding_mask}) # Q=K=V\n",
    "    \n",
    "    # 2-2. Dropout + Residual Connection + Layer Normalization\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout_ratio)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
    "    \n",
    "    # 3. Position-Wise Feed Forward Neural Networks (fully connected FFNN)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    \n",
    "    # 3-2. Dropout + Residual Connection + Layer Normalization\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout_ratio)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, encoder_outputs, look_ahead_mask, padding_mask], \n",
    "        outputs=outputs, \n",
    "        name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T05:01:31.988265Z",
     "start_time": "2023-12-17T05:01:31.981275Z"
    }
   },
   "id": "bfceafea6d0ddc32"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, dff, d_model, num_heads, dropout_ratio, name='decoder'):\n",
    "    \"\"\"Decoder\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: vocab size\n",
    "        num_layers: number of layers\n",
    "        dff: hidden layer size\n",
    "        d_model: embedding size\n",
    "        num_heads: number of heads\n",
    "        dropout_ratio: dropout ratio\n",
    "        name: decoder name\n",
    "    \n",
    "    Returns:\n",
    "        output: output of decoder\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    encoder_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    \n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    # 1. Embedding\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) # scale. 그래디언트 배니싱 문제를 완화하는 테크닉 (from 공식 구현체?)\n",
    "    \n",
    "    # 2. Positional Encoding + Dropout\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout_ratio)(embeddings)\n",
    "    \n",
    "    # 3. Stacking Decoder Layers by num_layers\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout_ratio=dropout_ratio, name='decoder_layer_{}'.format(i),)(inputs=[outputs, encoder_outputs, look_ahead_mask, padding_mask])\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, encoder_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T05:01:32.713849Z",
     "start_time": "2023-12-17T05:01:32.693625Z"
    }
   },
   "id": "237ff2b4fceb75d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
