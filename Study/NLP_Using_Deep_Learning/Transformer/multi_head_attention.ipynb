{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-16T13:04:03.430684Z",
     "start_time": "2023-12-16T13:04:01.394568Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    \n",
    "    depth : d_model / num_heads\n",
    "    ... : batch_size\n",
    "    \n",
    "    Args:\n",
    "        query: query shape == (..., num_heads, seq_len_q, depth)\n",
    "        key: key shape == (..., num_heads, seq_len_k, depth)\n",
    "        value: value shape == (..., num_heads, seq_len_v, depth)     \n",
    "        mask : mask shape == (..., 1, 1, seq_len_k)   \n",
    "        \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(a=query, b=key, transpose_b=True)  # Q*K while K is transposed. (..., num_heads, seq_len_q, seq_len_k)\n",
    "    depth_float = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    attention_logits = matmul_qk / tf.math.sqrt(depth_float)  # scale matmul_qk\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        attention_logits += (mask * -1e9)  # -1e9 : -infinite\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    # calculate the attention weights(== attention distribution).\n",
    "    attention_weights = tf.nn.softmax(attention_logits, axis=-1)  # (..., num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    attention_values = tf.matmul(attention_weights, value)  # (..., num_heads, seq_len_q, depth)\n",
    "\n",
    "    return attention_values, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T13:04:03.916873Z",
     "start_time": "2023-12-16T13:04:03.913530Z"
    }
   },
   "id": "ba1e7c3e2b7c39d0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        # make sure d_model can be divided by num_heads\n",
    "        assert d_model % self.num_heads == 0 \n",
    "        self.depth = d_model // self.num_heads # // : floor division\n",
    "        \n",
    "        # WQ, WK, WV\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        # WO\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "    \n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        \"\"\"Split Query, Key, Value with num_heads\n",
    "        \n",
    "        Args:\n",
    "            inputs: input shape == (batch_size, seq_len, d_model)\n",
    "            batch_size: batch size\n",
    "        \n",
    "        Returns:\n",
    "            result: result shape == (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        inputs = tf.reshape(tensor=inputs, shape=(batch_size, -1, self.num_heads, self.depth)) # (batch_size, seq_len, num_heads, depth)\n",
    "        return tf.transpose(a=inputs, perm=[0, 2, 1, 3]) # (batch_size, num_heads, seq_len, depth)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"MultiHeadAttention\n",
    "        \n",
    "        Args:\n",
    "            inputs: Q, K, V, mask\n",
    "                Q shape == (batch_size, seq_len_q, d_model)\n",
    "                K shape == (batch_size, seq_len_k, d_model)\n",
    "                V shape == (batch_size, seq_len_v, d_model)\n",
    "                mask shape == (batch_size, seq_len_q, seq_len_k)\n",
    "        \n",
    "        Returns:\n",
    "            output, attention_weights\n",
    "        \"\"\"\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(input=query)[0]\n",
    "        \n",
    "        # 1. Q,K,V linear layer\n",
    "        query = self.query_dense(query) # (batch_size, seq_len_q, d_model)\n",
    "        key = self.key_dense(key) # (batch_size, seq_len_k, d_model)\n",
    "        value = self.value_dense(value) # (batch_size, seq_len_v, d_model)\n",
    "        \n",
    "        # 2. split heads\n",
    "        query = self.split_heads(query, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        key = self.split_heads(key, batch_size) # (batch_size, num_heads, seq_len_k, depth)\n",
    "        value = self.split_heads(value, batch_size) # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # 3. scaled dot-product attention\n",
    "        temp_attention_values, attention_weights = scaled_dot_product_attention(query, key, value, mask) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        \n",
    "        # 4. transpose result and concat heads\n",
    "        temp_attention_values = tf.transpose(a=temp_attention_values, perm=[0, 2, 1, 3]) # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_temp_attention_values = tf.reshape(tensor=temp_attention_values, shape=(batch_size, -1, self.d_model)) # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        # 5. final linear layer\n",
    "        attention_values = self.dense(concat_temp_attention_values) # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return attention_values, attention_weights\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T13:04:04.713816Z",
     "start_time": "2023-12-16T13:04:04.692892Z"
    }
   },
   "id": "97ddb1dc361ee55f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tf.reshape\n",
    " - when dimension is -1, it means 'unspecified' dimension.\n",
    " - when dimension is -1, calculate this dimension automatically based on the size of the input and the other dimensions\". Itâ€™s a way of saying \"reshape this into whatever dimension is needed so that the total size remains constant\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa50abdebccba304"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def create_mask(x):\n",
    "    \"\"\"Create mask for padding\n",
    "    \n",
    "    Args:\n",
    "        x: input sequence\n",
    "        \n",
    "    Returns:\n",
    "        mask: mask for padding\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32) # 0 is padding value and find it.\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T13:15:41.460229Z",
     "start_time": "2023-12-16T13:15:41.455737Z"
    }
   },
   "id": "ec74ebf60ab3c765"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0.]]]], shape=(3, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_mask(tf.constant([[1, 2, 3, 0, 0], [0,0,0,0,0], [1,2,3,4,5]])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T13:16:04.020550Z",
     "start_time": "2023-12-16T13:16:04.014945Z"
    }
   },
   "id": "ae6aaa0eecad3c1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
