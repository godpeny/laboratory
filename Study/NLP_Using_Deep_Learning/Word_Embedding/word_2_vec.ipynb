{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-22T10:00:53.794533Z",
     "start_time": "2023-10-22T10:00:53.738940Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "525eaaa191eb1ec6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "('../data/ted_en-20160408.xml', <http.client.HTTPMessage at 0x2a636bc90>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download raw data\n",
    "download_path = \"../data/\"\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=download_path+\"ted_en-20160408.xml\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T09:23:57.693993Z",
     "start_time": "2023-10-22T09:23:52.573031Z"
    }
   },
   "id": "c2ee7bf2aece4f82"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# load xml file with tree structure and parse text\n",
    "xml = open(download_path+\"ted_en-20160408.xml\", \"r\", encoding=\"UTF8\")\n",
    "xml = etree.parse(xml)\n",
    "parse_text = '\\n'.join(xml.xpath('//content/text()'))\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text) # remove texts in parentheses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T09:46:26.437422Z",
     "start_time": "2023-10-22T09:46:26.119026Z"
    }
   },
   "id": "ac7bb92346c3b2c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### etree\n",
    "The ``xpath`` method is used to query the XML structure. \n",
    "This particular XPath query ``//content/text()`` selects the text content of all <content> tags in the XML document.\n",
    "\n",
    "### re\n",
    "``re.sub(pattern, replacement, string)``\n",
    "The re.sub function is used to find all substrings in a given string ``string`` that match a particular pattern ``pattern`` and then replace them with a different string ``replacement``.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b23bcbb81f32c2fc"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/godpeny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here are two reasons companies fail  they only do more of the same  or they only do what s new ', 'to me the real  real solution to quality growth is figuring out the balance between two activities  exploration and exploitation ', 'both are necessary  but it can be too much of a good thing ']\n"
     ]
    }
   ],
   "source": [
    "# tokenize sentences\n",
    "nltk.download('punkt') # download punkt tokenizer\n",
    "# sentence tokenization : sent_text is list of sentences\n",
    "sent_text = sent_tokenize(content_text) \n",
    "# normalize sentences : remove all except alphabet and number\n",
    "sent_text_normalized = []\n",
    "for s in sent_text:\n",
    "    token = re.sub(\"[^a-zA-Z0-9]\", \" \", s.lower()) \n",
    "    sent_text_normalized.append(token)\n",
    "    \n",
    "print(sent_text_normalized[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T09:53:44.541090Z",
     "start_time": "2023-10-22T09:53:41.833764Z"
    }
   },
   "id": "f747ff2e9e9fc2c7"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273424\n",
      "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']]\n"
     ]
    }
   ],
   "source": [
    "# tokenize words\n",
    "word_tokenized_text = [word_tokenize(s) for s in sent_text_normalized]\n",
    "print(len(word_tokenized_text)) # number of sentences with tokenized words\n",
    "print(word_tokenized_text[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T09:55:37.283618Z",
     "start_time": "2023-10-22T09:55:25.670163Z"
    }
   },
   "id": "ec880316bfb425e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de652f540f25ea24"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=word_tokenized_text, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T10:04:55.965036Z",
     "start_time": "2023-10-22T10:04:48.796913Z"
    }
   },
   "id": "e584d2a17c0b0433"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word2Vec\n",
    "``Word2Vec(sentences, size, window, min_count, workers, sg)``\n",
    "- sentences : list of sentences\n",
    "- vector_size : dimension of embedding vector\n",
    "- window : window size of context words\n",
    "- min_count : minimum frequency of words (if less than min_count, ignore)\n",
    "- workers : number of threads\n",
    "- sg : 0 for ``CBOW``, 1 for ``Skip-gram``"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6105f9ded35796d1"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('boy', 0.9291726350784302), ('woman', 0.833076000213623), ('lady', 0.818193793296814), ('kid', 0.8125599026679993), ('baby', 0.7507525682449341), ('man', 0.7321241497993469), ('sister', 0.698284924030304), ('soldier', 0.6746154427528381), ('daughter', 0.6730027794837952), ('mary', 0.6709443926811218)]\n"
     ]
    }
   ],
   "source": [
    "# show similar words\n",
    "print(model.wv.most_similar(\"girl\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T10:05:36.492836Z",
     "start_time": "2023-10-22T10:05:36.447578Z"
    }
   },
   "id": "88883c0c41fafd39"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# save model\n",
    "model_path = \"../model/\"\n",
    "model.wv.save_word2vec_format(model_path+\"eng_w2v\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T10:08:46.354959Z",
     "start_time": "2023-10-22T10:08:45.676564Z"
    }
   },
   "id": "650cc3203c5ddc41"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 0.802236020565033), ('chair', 0.7937402129173279), ('leg', 0.7689682245254517), ('hat', 0.7485133409500122), ('mom', 0.7483894228935242), ('doctor', 0.7459036111831665), ('seat', 0.7310882210731506), ('uncle', 0.7277305722236633), ('wrist', 0.7218130230903625), ('nose', 0.7200853228569031)]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "loaded_model = KeyedVectors.load_word2vec_format(model_path+\"eng_w2v\")\n",
    "print(loaded_model.most_similar(\"dog\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T10:09:21.624358Z",
     "start_time": "2023-10-22T10:09:21.036189Z"
    }
   },
   "id": "e17e8d3231ce5cdc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "43dc02e5e2029866"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
