{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:13:21.400334Z",
     "start_time": "2023-10-26T15:13:18.273395Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7f27372a1b8aa4f"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print(len(documents))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:13:22.006510Z",
     "start_time": "2023-10-26T15:13:21.402261Z"
    }
   },
   "id": "dac22b8cca42ae44"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           documents\n0  Well i'm not sure about the story nad it did s...\n1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n2  Although I realize that principle is not one o...\n3  Notwithstanding all the legitimate fuss about ...\n4  Well, I will have to change the scoring on my ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>documents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Well i'm not sure about the story nad it did s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Although I realize that principle is not one o...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Notwithstanding all the legitimate fuss about ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Well, I will have to change the scoring on my ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(documents, columns=['documents'])\n",
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:13:22.010762Z",
     "start_time": "2023-10-26T15:13:22.005447Z"
    }
   },
   "id": "ff482cf75b8cf1f4"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           documents  \\\n0  Well i'm not sure about the story nad it did s...   \n1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n2  Although I realize that principle is not one o...   \n\n                                           clean_doc  \n0  well sure about story seem biased. what disagr...  \n1  yeah, expect people read faq, etc. actually ac...  \n2  although realize that principle your strongest...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>documents</th>\n      <th>clean_doc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Well i'm not sure about the story nad it did s...</td>\n      <td>well sure about story seem biased. what disagr...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n      <td>yeah, expect people read faq, etc. actually ac...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Although I realize that principle is not one o...</td>\n      <td>although realize that principle your strongest...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "df[\"clean_doc\"] = df[\"documents\"].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# https://www.w3schools.com/python/python_lambda.asp\n",
    "df[\"clean_doc\"] = df[\"clean_doc\"].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) # remove words less than 3\n",
    "df[\"clean_doc\"] = df[\"clean_doc\"].apply(lambda x: x.lower()) # lower case\n",
    "df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:13:22.189866Z",
     "start_time": "2023-10-26T15:13:22.018817Z"
    }
   },
   "id": "7d9f4e8a32fc421c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "11004\n"
     ]
    }
   ],
   "source": [
    "# check nullable\n",
    "print(df.isnull().values.any())\n",
    "# In Python's numpy library (which pandas is built upon), NaN is used to denote missing or undefined data.\n",
    "# inPlace=True means that the changes are saved to the df right away (without having to assign it to another variable)\n",
    "df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "print(df.isnull().values.any())\n",
    "df.dropna(inplace=True)\n",
    "print(len(df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:13:22.198337Z",
     "start_time": "2023-10-26T15:13:22.189654Z"
    }
   },
   "id": "5b685beb09194e1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\r\n",
      "[nltk_data] Downloading package stopwords to\r\n",
      "[nltk_data]     /Users/godpeny/nltk_data...\r\n",
      "[nltk_data]   Package stopwords is already up-to-date!\r\n"
     ]
    }
   ],
   "source": [
    "# download stopwords\n",
    "!python -m nltk.downloader stopwords\n",
    "stopwords = stopwords.words('english')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:13:22.988448Z",
     "start_time": "2023-10-26T15:13:22.195692Z"
    }
   },
   "id": "93648e5bfc588e17"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [well, sure, story, seem, biased., disagree, s...\n",
      "1    [yeah,, expect, people, read, faq,, etc., actu...\n",
      "2    [although, realize, principle, strongest, poin...\n",
      "3    [notwithstanding, legitimate, fuss, proposal,,...\n",
      "4    [well,, change, scoring, playoff, pool., unfor...\n",
      "Name: clean_doc, dtype: object\n",
      "11004\n"
     ]
    }
   ],
   "source": [
    "# apply stopwords\n",
    "tokenized_doc = df['clean_doc'].apply(lambda x: x.split()) # tokenization\n",
    "tokenized_doc = df['clean_doc'].apply(lambda x: [w for w in x.split() if w not in stopwords]) # remove stop-words\n",
    "print(tokenized_doc[:5])\n",
    "print(len(tokenized_doc))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:13:24.340434Z",
     "start_time": "2023-10-26T15:13:23.032708Z"
    }
   },
   "id": "f4b3ec05a7d10ae"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 353, 486, 1224, 1653, 2323, 2373, 2864, 3292, 3389, 3397, 3398, 3562, 3567, 3595, 3784, 3879, 4185, 4592, 4622, 4952, 4975, 5395, 5530, 6020, 6657, 6728, 6888, 7085, 7961, 8161, 8288, 8422, 8594, 8627, 9703, 10283, 10447, 10738, 10758, 10904, 10916, 10964]\n",
      "10961\n"
     ]
    }
   ],
   "source": [
    "short_sentence_indices = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
    "print(short_sentence_indices)\n",
    "tokenized_doc = np.delete(tokenized_doc, short_sentence_indices)\n",
    "print(len(tokenized_doc))\n",
    "\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.Series.to_list.html\n",
    "# tokenized_doc = tokenized_doc.to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:13:24.344918Z",
     "start_time": "2023-10-26T15:13:24.341741Z"
    }
   },
   "id": "a2a36b4371e0d79c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40, 53, 927, 143, 15889, 1684, 546, 279, 871, 12028, 17773, 24007, 29726, 279, 871, 63435, 871, 1128, 1103, 1998, 851, 29727, 913, 731, 20477, 279, 871, 170, 143, 1811, 149, 279, 20478, 17773, 6645, 5710, 76, 63436, 7, 36, 165, 614, 653, 29728, 6911, 24008, 2082, 829, 17774, 1119, 8790, 355, 1072, 15890, 671, 57, 163, 4231, 7206, 1933, 440, 56, 282, 4730, 9275, 2690, 39306], [1283, 429, 3, 52, 6164, 159, 112, 474, 89, 17775, 18, 63, 4731, 2865, 63437, 1042, 402, 39307, 8791, 902, 44, 8328, 316, 13041, 902, 3452, 5923, 533, 18, 87, 4732, 9872, 160, 1403, 120, 151, 5194, 63438, 63439, 17776, 63440, 13041, 903, 63441, 63442, 11172, 17777], [249, 851, 2773, 9276, 4033, 1, 26, 2, 5, 95, 295, 904, 5711, 17, 655, 7, 2549, 63443, 6165, 39308, 311, 30, 13042, 36, 151, 484, 295, 280, 904, 1204, 415, 851, 1, 1217, 904, 63444, 1431, 282, 35, 86, 4591, 39308, 311, 1, 1150, 56, 2, 4335, 743, 312, 152, 11173, 1192, 10475, 656, 15891, 128, 4127, 123, 20479, 14287, 2423], [29729, 1484, 15892, 8329, 13, 177, 63445, 35, 657, 379, 5712, 29730, 905, 20480, 2866, 140, 4034, 2152, 63446, 57, 7520, 1627, 2288, 2867, 29731, 63447, 39309, 17778, 480, 69, 12029, 63448, 90, 4733, 273, 370, 1251, 79, 333, 6397, 3080, 4035, 63449, 8330, 999, 23, 1395, 6912, 1339, 24009, 347, 60, 1403, 1137, 8792, 110, 95, 96, 63450, 5528, 971, 598, 345, 129, 497, 6398, 17779, 6646, 324, 9, 3453, 2458, 46, 63451, 63452, 29732, 13043], [87, 177, 1970, 2083, 12030, 1883, 15, 29, 246, 238, 156, 1970, 678, 7921, 15893, 2289, 63453, 12031, 2730, 6913, 398, 7207, 398, 7208, 7209, 458, 7210]]\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "\n",
    "word_2_idx = tokenizer.word_index\n",
    "idx_2_word = {v:k for k, v in word_2_idx.items()} # key is index, value is word\n",
    "encoded = tokenizer.texts_to_sequences(tokenized_doc) \n",
    "print(encoded[:5])\n",
    "\n",
    "vocab_size = len(word_2_idx) + 1 # index 0 is for padding"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:13:25.011565Z",
     "start_time": "2023-10-26T15:13:24.400736Z"
    }
   },
   "id": "7ecbfcb7bf8de39c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Negative Sampling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "428d45fbb6d5f434"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import skipgrams"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:13:25.015748Z",
     "start_time": "2023-10-26T15:13:25.011490Z"
    }
   },
   "id": "add99a4f2313f20d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treating (4730), deport (93025)) -> 0\n",
      "(u.s. (279), pro-israeli (63435)) -> 1\n",
      "(inhuman (17774), rec.nude (139872)) -> 0\n",
      "(might (36), kazhakstan, (149379)) -> 0\n",
      "(government (57), mtm+2tm+2<7$9&1fpl%-3[>wm[8n+-#3%q<5g#tq,3$q,b8f)b<g)r186%a86 (172327)) -> 0\n",
      "10\n",
      "2460\n",
      "2460\n"
     ]
    }
   ],
   "source": [
    "# test with samples\n",
    "skip_grams = [skipgrams(sequence=sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]\n",
    "# show relationship\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "        idx_2_word[pairs[i][0]], pairs[i][0],\n",
    "        idx_2_word[pairs[i][1]], pairs[i][1],\n",
    "        labels[i]))\n",
    "    \n",
    "print(len(skip_grams))\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:13:25.049084Z",
     "start_time": "2023-10-26T15:13:25.034631Z"
    }
   },
   "id": "6940c60d38212015"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# do with all datasets\n",
    "skip_grams = [skipgrams(sequence=sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:14:16.887059Z",
     "start_time": "2023-10-26T15:13:25.190573Z"
    }
   },
   "id": "d7721f75406877b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "569518bc67a2f4b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers import Dot\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:14:16.891755Z",
     "start_time": "2023-10-26T15:14:16.887625Z"
    }
   },
   "id": "ec291911b20db3c2"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 100)\n",
      "(None, 1, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 100\n",
    "\n",
    "# embedding table for word\n",
    "w_input = Input(shape=(1,), dtype='int32')\n",
    "word_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dims)(w_input)\n",
    "\n",
    "# embedding table for context\n",
    "c_input = Input(shape=(1,), dtype='int32')\n",
    "context_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dims)(c_input)\n",
    "\n",
    "print(word_embedding.shape) # check shape\n",
    "print(context_embedding.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:14:16.961579Z",
     "start_time": "2023-10-26T15:14:16.892382Z"
    }
   },
   "id": "bc5557f582173b08"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2  3  4  5]\n",
      "  [ 6  7  8  9 10 11]]]\n",
      "[[[12 13]\n",
      "  [14 15]\n",
      "  [16 17]\n",
      "  [18 19]\n",
      "  [20 21]\n",
      "  [22 23]]]\n",
      "tf.Tensor(\n",
      "[[[290 902]\n",
      "  [305 953]]], shape=(1, 2, 2), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Practice : Dot product with 2 embedding tables\n",
    "\"\"\"\n",
    "x = np.arange(12).reshape(1, 2, 6)\n",
    "print(x)\n",
    "\n",
    "y = np.arange(12, 24).reshape(1, 6, 2)\n",
    "print(y)\n",
    "\n",
    "result = Dot(axes=(1, 2))([y,x]) # x * y\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:14:16.965736Z",
     "start_time": "2023-10-26T15:14:16.960796Z"
    }
   },
   "id": "10f15719cb86015b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! python -m pip install pydot\n",
    "! python -m pip install graphviz"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80091fa6e794abd7"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 1, 100)               1818390   ['input_1[0][0]']             \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 1, 100)               1818390   ['input_2[0][0]']             \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                 (None, 1, 1)                 0         ['embedding[0][0]',           \n",
      "                                                                     'embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1)                    0         ['dot_1[0][0]']               \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 1)                    0         ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 36367800 (138.73 MB)\n",
      "Trainable params: 36367800 (138.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "output = Activation('sigmoid')(dot_product)\n",
    "\n",
    "model = Model(inputs=[w_input, c_input], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "plot_model(model, to_file='sgns.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-26T15:14:16.966518Z"
    }
   },
   "id": "1e481e3ab74bb9ab"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 5588.079095847905\n",
      "Epoch : 2 Loss : 4304.712773874402\n",
      "Epoch : 3 Loss : 4064.503253623843\n",
      "Epoch : 4 Loss : 3770.21881897375\n",
      "Epoch : 5 Loss : 3455.9627989614382\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6): \n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32') # zip(*elem[0]) : transpose\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        \n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X,Y)\n",
    "    print('Epoch :',epoch, 'Loss :',loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:59:29.342079Z",
     "start_time": "2023-10-26T15:14:17.041935Z"
    }
   },
   "id": "41f1c4f69cd8d27e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save Embedding Vectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36301f711bc3bf0c"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import gensim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T16:00:26.167370Z",
     "start_time": "2023-10-26T16:00:26.050824Z"
    }
   },
   "id": "6e590828520b971b"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# save embedding vectors\n",
    "f = open('./embeddings/vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embedding_dims))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T16:03:04.844346Z",
     "start_time": "2023-10-26T16:02:59.819380Z"
    }
   },
   "id": "f778044948b6df24"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# load embedding vectors\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./embeddings/vectors.txt', binary=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T16:09:22.143096Z",
     "start_time": "2023-10-26T16:09:17.425564Z"
    }
   },
   "id": "85b6fb0da5c104a7"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "[('villages', 0.7206360697746277),\n ('lebanese', 0.6771818995475769),\n ('shelling', 0.6653899550437927),\n ('arab', 0.6578678488731384),\n ('occupied', 0.6578179597854614),\n ('murdered', 0.6374198794364929),\n ('ottoman', 0.6369696259498596),\n ('lebanon', 0.6363499164581299),\n ('destruction', 0.6337578296661377),\n ('israelis', 0.6337222456932068)]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['soldiers'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T16:09:30.704033Z",
     "start_time": "2023-10-26T16:09:30.581449Z"
    }
   },
   "id": "e1b960eb01fa6860"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dae6fdb7123aff37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
