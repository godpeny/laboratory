{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-02T09:46:35.828988Z",
     "start_time": "2023-12-02T09:46:35.824299Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193340\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                 어릴때보고 지금다시봐도 재밌어요ㅋㅋ\n0  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...\n1               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.\n2  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...\n3                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.\n4                      사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../data/\"\n",
    "data_name = data_path + \"ratings_processed.txt\"\n",
    "df = pd.read_table(data_path + data_name) # load processed data\n",
    "\n",
    "print(len(df))\n",
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T09:24:20.218235Z",
     "start_time": "2023-12-02T09:24:19.938678Z"
    }
   },
   "id": "2397aed98a7294dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bcee83a2df81fbe"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False) # no lowercase, no accent stripping\n",
    "# train token\n",
    "tokenizer.train(files=data_name, vocab_size=30000, limit_alphabet=6000, min_frequency=5)\n",
    "# save model\n",
    "model_path = \"../model/\"\n",
    "model_name = \"bert_tokenizer_vocab.txt\"\n",
    "tokenizer.save_model(model_path)\n",
    "\n",
    "# rename model\n",
    "os.rename(model_path + \"vocab.txt\", model_path + model_name) # change name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T09:49:41.124273Z",
     "start_time": "2023-12-02T09:49:37.630702Z"
    }
   },
   "id": "14a80f4c2df9dd33"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "data": {
      "text/plain": "        0\n0   [PAD]\n1   [UNK]\n2   [CLS]\n3   [SEP]\n4  [MASK]\n5       !\n6       \"\n7       #\n8       $\n9       %",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[PAD]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[UNK]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[CLS]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[SEP]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[MASK]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>!</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>\"</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>$</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>%</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "df = pd.read_fwf(filepath_or_buffer=model_path+model_name, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T09:50:53.222276Z",
     "start_time": "2023-12-02T09:50:53.181715Z"
    }
   },
   "id": "c97f0e6ff716938a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using Bert Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ad88903eb558975"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 : ['아', '배고', '##픈', '##데', '짜장면', '먹고', '##싶다']\n",
      "정수 인코딩 : [2111, 20597, 3343, 3309, 24621, 8675, 7372]\n",
      "디코딩 : 아 배고픈데 짜장면 먹고싶다\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('아 배고픈데 짜장면 먹고싶다')\n",
    "print('토큰화 결과 :',encoded.tokens)\n",
    "print('정수 인코딩 :',encoded.ids)\n",
    "print('디코딩 :',tokenizer.decode(encoded.ids))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T09:51:56.128679Z",
     "start_time": "2023-12-02T09:51:56.124939Z"
    }
   },
   "id": "2fb50bf4ab112793"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Other Tokenizer\n",
    " - BertWordPieceTokenizer : BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer)\n",
    " - CharBPETokenizer : 오리지널 BPE\n",
    " - ByteLevelBPETokenizer : BPE의 바이트 레벨 버전\n",
    " - SentencePieceBPETokenizer : 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80b40fa0048813a9"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T09:53:08.060363Z",
     "start_time": "2023-12-02T09:53:08.043266Z"
    }
   },
   "id": "653593a77e58784"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
