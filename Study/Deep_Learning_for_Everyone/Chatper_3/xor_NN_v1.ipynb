{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOSPAmbMCDCSFK8CwDSN3Dx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/godpeny/laboratory/blob/master/Study/Deep_Learning_for_Everyone/Chatper_3/xor_NN_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d_jsX45VtaKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1c8bb3-e23d-416d-8d13-8ad4ead309dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "# insall\n",
        "%pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import numpy as np\n",
        "import random as rand"
      ],
      "metadata": {
        "id": "H47pnEk5t5FZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    [[0,0],[0]],\n",
        "    [[0,1],[1]],\n",
        "    [[1,0],[1]],\n",
        "    [[1,1],[0]],\n",
        "]\n",
        "\n",
        "epochs = 5000\n",
        "lr = 0.1\n",
        "mo = 0.4 # momentum for momentum SGD"
      ],
      "metadata": {
        "id": "k2gyWj6svGMN"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functions\n",
        "# activation functions\n",
        "def sigmoid(x, deriv=False):\n",
        "  \"\"\"\n",
        "  sigmoid function\n",
        "  \"\"\"\n",
        "  if deriv==False:\n",
        "    return 1 / (1 + np.exp(-x)) # exponential of Euler's number\n",
        "  else:\n",
        "    return sigmoid(x, False) * (1 - sigmoid(x, False))\n",
        "    # return x * (1-x)\n",
        "\n",
        "def tanh(x, deriv=False):\n",
        "  \"\"\"\n",
        "  hyperbolic tangent\n",
        "  \"\"\"\n",
        "  if deriv==False:\n",
        "    # return (np.exp(x) - np.exp(-x) / np.exp(x) + np.exp(-x)) # equals to np.tanh(x)\n",
        "    return np.tanh(x)\n",
        "  else:\n",
        "    return (1 - (tanh(x, False)**2))\n",
        "    # return (1 - (x**2))\n",
        "\n",
        "# general functions\n",
        "def makeMatrix(col, row, fill=0.0):\n",
        "  \"\"\"\n",
        "  intialize matrix with col*row with value of fill\n",
        "  \"\"\"\n",
        "  matrix = []\n",
        "  for i in range(col):\n",
        "    matrix.append([fill] * row)\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "MhRv0yLVwA3h"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NN\n",
        "class NN:\n",
        "  \"\"\"\n",
        "  Class for Neural Network\n",
        "  \"\"\"\n",
        "  def __init__(self, num_x, num_yh, num_yo, bias=1):\n",
        "    \"\"\"\n",
        "    constructor of the class\n",
        "    num_x : input x\n",
        "    num_yh : hidden layer output\n",
        "    num_yo : output\n",
        "    \"\"\"\n",
        "\n",
        "    self.bias = bias\n",
        "    self.num_x = num_x + self.bias;\n",
        "    self.num_yh = num_yh\n",
        "    self.num_yo = num_yo\n",
        "\n",
        "    # initialize activation variables\n",
        "    self.activation_input = [1.0] * self.num_x\n",
        "    self.activation_hidden = [1.0] * self.num_yh\n",
        "    self.activation_output = [1.0] * self.num_yo\n",
        "\n",
        "    # initialize weight in\n",
        "    self.weight_in = makeMatrix(self.num_x, self.num_yh)\n",
        "    for i in range(self.num_x):\n",
        "      for j in range(self.num_yh):\n",
        "        self.weight_in[i][j] = rand.random()\n",
        "\n",
        "    # initialize weight out\n",
        "    self.weight_out = makeMatrix(self.num_yh, self.num_yo)\n",
        "    for j in range(self.num_yh):\n",
        "      for k in range(self.num_yo):\n",
        "        self.weight_out[j][k] = rand.random()\n",
        "\n",
        "    # initialize momentum SGD\n",
        "    self.gradient_in = makeMatrix(self.num_x, self.num_yh)\n",
        "    self.gradient_out = makeMatrix(self.num_yh, self.num_yo)\n",
        "\n",
        "  def update(self, inputs):\n",
        "    # update input layer activation function\n",
        "    for i in range(self.num_x - self.bias):\n",
        "      self.activation_input[i] = inputs[i]\n",
        "\n",
        "    # update hidden layer activation function\n",
        "    for j in range(self.num_yh):\n",
        "      sum = 0.0\n",
        "      for i in range(self.num_x):\n",
        "        sum = sum + self.activation_input[i] * self.weight_in[i][j]\n",
        "      self.activation_hidden[j] = tanh(sum, False)\n",
        "\n",
        "    # update output layer activation function\n",
        "    for k in range(self.num_yo):\n",
        "      sum = 0.0\n",
        "      for j in range(self.num_yh):\n",
        "        sum = sum + self.activation_hidden[j] * self.weight_out[j][k]\n",
        "      self.activation_output[k] = tanh(sum, False)\n",
        "\n",
        "    return self.activation_output[:] # shallow copy\n",
        "\n",
        "  def back_propagation(self, targets):\n",
        "    \"\"\"\n",
        "    Delta Rule\n",
        "    https://en.wikipedia.org/wiki/Delta_rule\n",
        "\n",
        "    error = (target - output)\n",
        "    deltas = (derivation of activation function) * error\n",
        "\n",
        "    Stochastic Gradient Descent with momentum\n",
        "    http://aikorea.org/cs231n/neural-networks-3/#sgd\n",
        "\n",
        "    Momentum update\n",
        "    velocity = mu * v - learning_rate * dx\n",
        "    x += velocity\n",
        "    \"\"\"\n",
        "\n",
        "    # output deltas\n",
        "    output_deltas = [0.0] * self.num_yo\n",
        "    for k in range(self.num_yo):\n",
        "      err = targets[k] - self.activation_output[k]\n",
        "      output_deltas[k] = err * tanh(self.activation_output[k], True)\n",
        "\n",
        "    # hidden layer deltas\n",
        "    hidden_deltas = [0.0] * self.num_yh\n",
        "    for j in range(self.num_yh):\n",
        "      err = 0.0\n",
        "      for k in range(self.num_yo):\n",
        "        err = err + output_deltas[k] * self.weight_out[j][k]\n",
        "      hidden_deltas[j] = err * tanh(self.activation_hidden[j], True)\n",
        "\n",
        "    # update gradient out\n",
        "    for j in range(self.num_yh):\n",
        "      for k in range(self.num_yo):\n",
        "        gradient = output_deltas[k] * self.activation_hidden[j]\n",
        "        velocity = mo * self.gradient_out[j][k] - (lr * gradient)\n",
        "        self.weight_out[j][k] += velocity\n",
        "        self.gradient_out[j][k] = gradient\n",
        "\n",
        "    # update gradient in\n",
        "    for i in range(self.num_x):\n",
        "      for j in range(self.num_yh):\n",
        "        gradient = hidden_deltas[j] * self.activation_input[i]\n",
        "        velocity = mo * self.gradient_in[i][j] - (lr * gradient)\n",
        "        self.weight_in[i][j] += velocity\n",
        "        self.gradient_in[i][j] = gradient\n",
        "\n",
        "    # mean square error\n",
        "    error = 0.0\n",
        "    for i in range(len(targets)):\n",
        "      error = error + (1/2) * ((targets[i] - self.activation_output[i])**2)\n",
        "    return error\n",
        "\n",
        "  def train(self, patterns):\n",
        "    for i in range(epochs):\n",
        "      error = 0.0\n",
        "      for p in patterns:\n",
        "        inputs = p[0]\n",
        "        targets = p[1]\n",
        "        # update input\n",
        "        self.update(inputs)\n",
        "        # back propagation\n",
        "        error = error + self.back_propagation(targets)\n",
        "      if i % 500 == 0:\n",
        "        print('error: %-.5f' % error)\n",
        "\n",
        "  def result(self, patterns):\n",
        "    for p in patterns:\n",
        "      print('Input: %s, Predict: %s' % (p[0], self.update(p[0])))"
      ],
      "metadata": {
        "id": "dS4VJDvJy4ZO"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main\n",
        "nn = NN(2,2,1)\n",
        "\n",
        "nn.train(data)\n",
        "\n",
        "nn.result(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW5eaNynPuoD",
        "outputId": "940083c5-8924-45a0-ec09-62cf72d3edd2"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: 0.50922\n",
            "error: 0.03127\n",
            "error: 0.02056\n",
            "error: 0.00154\n",
            "error: 0.00135\n",
            "error: 0.00118\n",
            "error: 0.00129\n",
            "error: 0.00139\n",
            "error: 0.00151\n",
            "error: 0.00188\n",
            "Input: [0, 0], Predict: [0.037948898812758]\n",
            "Input: [0, 1], Predict: [0.9999804703490038]\n",
            "Input: [1, 0], Predict: [0.9999963631125318]\n",
            "Input: [1, 1], Predict: [0.07078124691477407]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. shouldn't derivative form of acivation function be like below?  \n",
        "\n",
        "sigmoid : ``sigmoid(x, False) * (1 - sigmoid(x, False))``\n",
        "\n",
        "tanh : ``(1 - (tanh(x, False)**2))``\n",
        "\n",
        "Q2. What is ``gradient_in`` and ``gradient_out`` ? isn't it for saving previous velocity in Stochastic Gradient Descent with momentum?\n",
        "\n"
      ],
      "metadata": {
        "id": "0RTf9nYsUTvl"
      }
    }
  ]
}