{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhDo+axrry7f5Vwf34JS3y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/godpeny/laboratory/blob/master/Study/Deep_Learning_for_Everyone/Chapter_5/embedding_for_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74sNxK2orh2x",
        "outputId": "95bbacd6-5035-4095-bcf0-f90b88f66dc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post9.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post9-py3-none-any.whl size=2952 sha256=a460c9bf226d09e6d2201c7877d40684cca651c6b4a9e8f5f3b608222463833f\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/a3/d2/092b519e9522b4c91608b7dcec0dd9051fa1bff4c45f4502d1\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post9\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# install\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install tensorflow\n",
        "%pip install sklearn\n",
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XLCFNVj50MZn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Tokenizer\n",
        "\"\"\"\n",
        "docs = [\n",
        "    \"Sometimes to understand a word's meaning you need more than a definition you need to see the word used in a sentence.\",\n",
        "    \"At YourDictionary, we give you the tools to learn what a word means and how to use it correctly.\",\n",
        "    \"With this sentence maker, simply type a word in the search bar and see a variety of sentences with that word used in its different ways.\",\n",
        "    \"Our sentence generator can provide more context and relevance, ensuring you use a word the right way.\"\n",
        "    ]\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "print(token.document_count)\n",
        "print(token.word_counts)\n",
        "print(token.word_docs) # count words on docs\n",
        "print(token.word_index) # index"
      ],
      "metadata": {
        "id": "5Ye0g-0O4EyA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d66542-dbfc-41a4-c987-b68d798458f0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "OrderedDict([('sometimes', 1), ('to', 4), ('understand', 1), ('a', 7), (\"word's\", 1), ('meaning', 1), ('you', 4), ('need', 2), ('more', 2), ('than', 1), ('definition', 1), ('see', 2), ('the', 4), ('word', 5), ('used', 2), ('in', 3), ('sentence', 3), ('at', 1), ('yourdictionary', 1), ('we', 1), ('give', 1), ('tools', 1), ('learn', 1), ('what', 1), ('means', 1), ('and', 3), ('how', 1), ('use', 2), ('it', 1), ('correctly', 1), ('with', 2), ('this', 1), ('maker', 1), ('simply', 1), ('type', 1), ('search', 1), ('bar', 1), ('variety', 1), ('of', 1), ('sentences', 1), ('that', 1), ('its', 1), ('different', 1), ('ways', 1), ('our', 1), ('generator', 1), ('can', 1), ('provide', 1), ('context', 1), ('relevance', 1), ('ensuring', 1), ('right', 1), ('way', 1)])\n",
            "defaultdict(<class 'int'>, {'a': 4, 'you': 3, 'need': 1, 'used': 2, 'more': 2, 'than': 1, 'understand': 1, \"word's\": 1, 'definition': 1, 'the': 4, 'word': 4, 'to': 2, 'sentence': 3, 'in': 2, 'sometimes': 1, 'meaning': 1, 'see': 2, 'yourdictionary': 1, 'means': 1, 'what': 1, 'and': 3, 'it': 1, 'tools': 1, 'how': 1, 'give': 1, 'at': 1, 'we': 1, 'correctly': 1, 'learn': 1, 'use': 2, 'this': 1, 'variety': 1, 'search': 1, 'that': 1, 'with': 1, 'its': 1, 'ways': 1, 'type': 1, 'of': 1, 'different': 1, 'maker': 1, 'simply': 1, 'bar': 1, 'sentences': 1, 'can': 1, 'right': 1, 'our': 1, 'ensuring': 1, 'relevance': 1, 'provide': 1, 'context': 1, 'way': 1, 'generator': 1})\n",
            "{'a': 1, 'word': 2, 'to': 3, 'you': 4, 'the': 5, 'in': 6, 'sentence': 7, 'and': 8, 'need': 9, 'more': 10, 'see': 11, 'used': 12, 'use': 13, 'with': 14, 'sometimes': 15, 'understand': 16, \"word's\": 17, 'meaning': 18, 'than': 19, 'definition': 20, 'at': 21, 'yourdictionary': 22, 'we': 23, 'give': 24, 'tools': 25, 'learn': 26, 'what': 27, 'means': 28, 'how': 29, 'it': 30, 'correctly': 31, 'this': 32, 'maker': 33, 'simply': 34, 'type': 35, 'search': 36, 'bar': 37, 'variety': 38, 'of': 39, 'sentences': 40, 'that': 41, 'its': 42, 'different': 43, 'ways': 44, 'our': 45, 'generator': 46, 'can': 47, 'provide': 48, 'context': 49, 'relevance': 50, 'ensuring': 51, 'right': 52, 'way': 53}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "One-hot-encodding\n",
        "\"\"\"\n",
        "text = \"Sometimes to understand a word's meaning you need more than a definition you need to see the word used in a sentence.\"\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts([text])\n",
        "print(token.word_index)\n",
        "\n",
        "x = token.texts_to_sequences([text]) # text is converted into its word index list\n",
        "print(x)\n",
        "\n",
        "word_len = len(token.word_index) + 1 # 'token.word_index' begins with 1 so make 0 index.\n",
        "x = to_categorical(x, num_classes=word_len)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "w764VDdh5wkd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e19e31-9ba7-42fa-d2af-13415f721615"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 1, 'to': 2, 'you': 3, 'need': 4, 'sometimes': 5, 'understand': 6, \"word's\": 7, 'meaning': 8, 'more': 9, 'than': 10, 'definition': 11, 'see': 12, 'the': 13, 'word': 14, 'used': 15, 'in': 16, 'sentence': 17}\n",
            "[[5, 2, 6, 1, 7, 8, 3, 4, 9, 10, 1, 11, 3, 4, 2, 12, 13, 14, 15, 16, 1, 17]]\n",
            "[[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Embedding\n",
        "\"\"\"\n",
        "# positive & negative reviews\n",
        "reviews = [\n",
        "  \"stunning visuals and a gripping performance\",\n",
        "  \"a cinematic masterpiece with a moving story\",\n",
        "  \"intriguing plot twists and impeccable acting\",\n",
        "  \"a visual treat with a compelling narrative\",\n",
        "  \"innovative, memorable, and utterly captivating\",\n",
        "  \"predictable plot and one-dimensional characters\",\n",
        "  \"lacks depth, originality, and excitement\",\n",
        "  \"unconvincing acting and poorly written scrip\",\n",
        "  \"the pacing was slow and the story, uninspiring\",\n",
        "  \"disappointing execution and lackluster performance\"\n",
        "]\n",
        "\n",
        "# evals 1 is positive and 0 is negative\n",
        "evals = np.array([1,1,1,1,1,0,0,0,0,0])\n",
        "\n",
        "# tokenize\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(reviews)\n",
        "print(token.word_index)\n",
        "\n",
        "# convert into index slice\n",
        "x = token.texts_to_sequences(reviews)\n",
        "print(x)\n",
        "\n",
        "# padding (data element length should be same to be processed)\n",
        "longest = 0;\n",
        "for i in x:\n",
        "  if len(i) > longest:\n",
        "    longest = len(i)\n",
        "\n",
        "padded_x = pad_sequences(x, longest)\n",
        "print(padded_x)\n",
        "\n",
        "# use embedding for modeling\n",
        "input_len = len(token.word_index) + 1\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=input_len, output_dim=8, input_length=longest)) # pick the value of output_dim.\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
        "\n",
        "# train model\n",
        "history = model.fit(padded_x, evals, epochs=20, batch_size=5)\n",
        "\n",
        "# evaluate model\n",
        "model_eval = model.evaluate(padded_x, evals)\n",
        "print(model_eval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQnD3Stl8WoR",
        "outputId": "4ae45dab-6df0-4c3b-b3a7-9749584ea7cc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'and': 1, 'a': 2, 'performance': 3, 'with': 4, 'story': 5, 'plot': 6, 'acting': 7, 'the': 8, 'stunning': 9, 'visuals': 10, 'gripping': 11, 'cinematic': 12, 'masterpiece': 13, 'moving': 14, 'intriguing': 15, 'twists': 16, 'impeccable': 17, 'visual': 18, 'treat': 19, 'compelling': 20, 'narrative': 21, 'innovative': 22, 'memorable': 23, 'utterly': 24, 'captivating': 25, 'predictable': 26, 'one': 27, 'dimensional': 28, 'characters': 29, 'lacks': 30, 'depth': 31, 'originality': 32, 'excitement': 33, 'unconvincing': 34, 'poorly': 35, 'written': 36, 'scrip': 37, 'pacing': 38, 'was': 39, 'slow': 40, 'uninspiring': 41, 'disappointing': 42, 'execution': 43, 'lackluster': 44}\n",
            "[[9, 10, 1, 2, 11, 3], [2, 12, 13, 4, 2, 14, 5], [15, 6, 16, 1, 17, 7], [2, 18, 19, 4, 2, 20, 21], [22, 23, 1, 24, 25], [26, 6, 1, 27, 28, 29], [30, 31, 32, 1, 33], [34, 7, 1, 35, 36, 37], [8, 38, 39, 40, 1, 8, 5, 41], [42, 43, 1, 44, 3]]\n",
            "[[ 0  0  9 10  1  2 11  3]\n",
            " [ 0  2 12 13  4  2 14  5]\n",
            " [ 0  0 15  6 16  1 17  7]\n",
            " [ 0  2 18 19  4  2 20 21]\n",
            " [ 0  0  0 22 23  1 24 25]\n",
            " [ 0  0 26  6  1 27 28 29]\n",
            " [ 0  0  0 30 31 32  1 33]\n",
            " [ 0  0 34  7  1 35 36 37]\n",
            " [ 8 38 39 40  1  8  5 41]\n",
            " [ 0  0  0 42 43  1 44  3]]\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, 8, 8)              360       \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 425 (1.66 KB)\n",
            "Trainable params: 425 (1.66 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "2/2 [==============================] - 1s 8ms/step - loss: 0.6858 - accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6800 - accuracy: 0.6000\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6747 - accuracy: 0.8000\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6698 - accuracy: 0.9000\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6646 - accuracy: 0.9000\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6593 - accuracy: 0.9000\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6541 - accuracy: 0.9000\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6491 - accuracy: 0.9000\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6438 - accuracy: 0.9000\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6386 - accuracy: 0.9000\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6333 - accuracy: 0.9000\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6281 - accuracy: 0.9000\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6227 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6176 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6120 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6065 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6009 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5953 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5895 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.5840 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.5793 - accuracy: 1.0000\n",
            "[0.579331636428833, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding\n",
        "## word embedding using elmo\n",
        "http://www.realworldnlpbook.com/blog/improving-sentiment-analyzer-using-elmo.html\n",
        "\n",
        "https://wikidocs.net/33930\n"
      ],
      "metadata": {
        "id": "gIBje3wcFMUi"
      }
    }
  ]
}